import streamlit as st

# Set page config - must be the first Streamlit command
st.set_page_config(page_title="CellChat Permutation Comparison", page_icon="ğŸ’¬", layout="wide")

import scanpy as sc
import numpy as np
import pandas as pd
import os
import pickle
import re
import time
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D
import seaborn as sns
import zipfile
import io
from joblib import Parallel, delayed
from scipy.stats import mannwhitneyu
from statsmodels.stats.multitest import multipletests
from tqdm import tqdm
import copy
import gc
from streamlit_sortables import sort_items
import logging
import traceback
import scipy
from pages.cellchat import (
    get_cellchatdb_from_r,  debug_cellchatdb, check_species_index, identify_overexpressed_genes, 
    preprocess_data,  computeExpr_LR,computeExpr_coreceptor, compute_hill_outer, computeExpr_agonist, 
    computeExpr_antagonist, precompute_gene_expressions, computeCommunProbPathway,
    aggregateCell_Cell_Communication
)

from pages.cellchat_vis import (
    netVisual_circle, netVisual_circle_individual, netVisual_chord_by_pathway,
    plotGeneExpression, plot_lr_contribution,
    netVisual_heatmap, create_chord_diagram_pycirclize, netAnalysis_signalingRole_scatter,
    netAnalysis_signalingChanges_scatter
)




# Create temp directory for saving files
if "cellchat_temp_dir" not in st.session_state:
    st.session_state.cellchat_temp_dir = True
    cellchat_temp_dir = "temp/" + str(round(time.time()))
    if not os.path.exists('temp'):
        os.mkdir('temp')
    os.mkdir(cellchat_temp_dir)
    st.session_state.cellchat_temp_dir = cellchat_temp_dir
else:
    cellchat_temp_dir = st.session_state.cellchat_temp_dir

# rpy2ã®è­¦å‘Šã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®å‰å‡¦ç†
def setup_rpy2_warning_fix():
    """rpy2ã®è­¦å‘Šã‚’æŠ‘åˆ¶ã™ã‚‹ãŸã‚ã®è¨­å®š"""
    # ç’°å¢ƒå¤‰æ•°è¨­å®š
    os.environ['RPY2_INITIALIZE_QUIET'] = "TRUE"
    
    try:
        # Rã®ãƒ¡ã‚¤ãƒ³åˆæœŸåŒ–ã‚’äº‹å‰ã«è¡Œã†
        import rpy2.robjects as ro
        from rpy2.rinterface_lib import callbacks
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›ã‚’æŠ‘åˆ¶
        callbacks.consolewrite_print = lambda x: None
        callbacks.consolewrite_warnerror = lambda x: None
        
        # åˆæœŸåŒ–æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
        if not ro.r:
            ro.r('library(methods)')
        
        return True
    except ImportError:
        # rpy2ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ä½•ã‚‚ã—ãªã„
        return False
    except Exception as e:
        print(f"RåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ (ç„¡è¦–å¯èƒ½): {str(e)}")
        return False

# GPUæƒ…å ±å–å¾—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
def get_gpu_memory_info():
    """GPU ãƒ¡ãƒ¢ãƒªæƒ…å ±å–å¾—"""
    try:
        import cupy as cp
        mem_info = cp.cuda.Device().mem_info
        total_mem = mem_info[1]
        free_mem = mem_info[0]
        used_mem = total_mem - free_mem
        return {
            "total_gb": total_mem / 1e9,
            "free_gb": free_mem / 1e9,
            "used_gb": used_mem / 1e9,
            "usage_percent": (used_mem / total_mem) * 100
        }
    except:
        return None


# 64ã‚³ã‚¢ç’°å¢ƒå‘ã‘NumPyã‚¹ãƒ¬ãƒƒãƒ‰åˆ¶é™
def limit_numpy_threads():
    """64ã‚³ã‚¢ç’°å¢ƒå‘ã‘ã«NumPyã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’åˆ¶é™"""
    # ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šï¼ˆ1ã‚¹ãƒ¬ãƒƒãƒ‰ã«åˆ¶é™ï¼‰
    os.environ["OMP_NUM_THREADS"] = "1"
    os.environ["MKL_NUM_THREADS"] = "1"
    os.environ["OPENBLAS_NUM_THREADS"] = "1"
    os.environ["NUMEXPR_NUM_THREADS"] = "1"
    os.environ["VECLIB_MAXIMUM_THREADS"] = "1"
    
    # NumPyã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’ç›´æ¥è¨­å®š
    try:
        import numpy as np
        np.set_num_threads(1)
    except:
        pass

# 64ã‚³ã‚¢ç’°å¢ƒå‘ã‘æœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚ºè¨ˆç®—
def determine_optimal_batch_size_64core(n_perms, n_jobs, n_cells, n_genes, has_gpu=False, gpu_mem=0):
    """64ã‚³ã‚¢ç’°å¢ƒå‘ã‘ã«æœ€é©åŒ–ã•ã‚ŒãŸãƒãƒƒãƒã‚µã‚¤ã‚ºè¨ˆç®—é–¢æ•°"""
    # 64ã‚³ã‚¢ç’°å¢ƒã§ã®åŸºæœ¬ãƒãƒƒãƒã‚µã‚¤ã‚º
    base_batch_size = max(8, n_jobs // 4)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«åŸºã¥ãèª¿æ•´
    data_size_factor = 1.0
    
    # å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å ´åˆã¯ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™
    if n_cells > 10000 or n_genes > 10000:
        data_size_factor = 0.7
    
    # ã•ã‚‰ã«å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãªã‚‰ã•ã‚‰ã«ç¸®å°
    if n_cells > 15000 or n_genes > 15000:
        data_size_factor = 0.5
    
    # GPUåˆ©ç”¨å¯èƒ½æ™‚ã®èª¿æ•´
    gpu_factor = 1.0
    if has_gpu and gpu_mem > 0:
        # 1ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚ãŸã‚Šã®æ¨å®šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆãƒã‚¤ãƒˆï¼‰
        # float64ï¼ˆ8ãƒã‚¤ãƒˆï¼‰Ã— 2.5å€ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰è¦‹ç©ã‚‚ã‚Š
        estimated_mem_per_perm = n_cells * n_genes * 8 * 2.5
        
        # GPUã§ä¸€åº¦ã«å‡¦ç†ã§ãã‚‹ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ•°ï¼ˆ70%ã®GPUãƒ¡ãƒ¢ãƒªã‚’ä½¿ç”¨å¯èƒ½ã¨ä»®å®šï¼‰
        max_gpu_perms = int((gpu_mem * 0.7) / estimated_mem_per_perm)
        
        # 64ã‚³ã‚¢ç’°å¢ƒã§ã¯å°‘ãªã‚ã«è¨­å®šï¼ˆCPUãŒå¼·åŠ›ãªãŸã‚ï¼‰
        max_gpu_perms = max(1, min(max_gpu_perms, n_jobs // 4))
        
        # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã•ã‚‰ã«åˆ¶é™
        if n_cells * n_genes > 100e6:  # 1å„„è¦ç´ è¶…
            max_gpu_perms = max(1, max_gpu_perms // 2)
            
        print(f"GPUåŒæ™‚å‡¦ç†å¯èƒ½ãªæ¨å®šãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ•° (float64ç²¾åº¦): {max_gpu_perms}")
        gpu_factor = min(1.0, max_gpu_perms / base_batch_size)
    
    # æœ€çµ‚ãƒãƒƒãƒã‚µã‚¤ã‚ºè¨ˆç®—
    batch_size = max(1, int(base_batch_size * data_size_factor * gpu_factor))
    
    # n_permsã‚’è¶…ãˆãªã„ã‚ˆã†ã«ã™ã‚‹
    batch_size = min(batch_size, n_perms)
    
    # float64ç²¾åº¦ä½¿ç”¨æ™‚ã¯ã•ã‚‰ã«ä¸Šé™ã‚’ä¸‹ã’ã‚‹
    # 64ã‚³ã‚¢ç’°å¢ƒã§ã¯å¤§ãã‚ã®ãƒãƒƒãƒã‚’è¨±å®¹ï¼ˆãƒ¡ãƒ¢ãƒªãŒååˆ†ã‚ã‚‹å‰æï¼‰
    batch_size = min(batch_size, 30)
    
    return batch_size

# ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è§£æã®ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆ64ã‚³ã‚¢ã€NUMAç„¡ã—ç‰ˆï¼‰
def run_permutation_cellchat_64core(adata1, adata2, groupby, db, cell_types, n_perm=100, n_jobs=48, 
                                    use_gpu=True, gpu_precision=True, hybrid_mode=True, gpu_ratio=0.3, 
                                    optimize_memory=True, **kwargs):
    """
    64ã‚³ã‚¢ç’°å¢ƒã«æœ€é©åŒ–ã•ã‚ŒãŸCellChatãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è§£æï¼ˆNUMAæœ€é©åŒ–ãªã—ï¼‰
    
    Parameters:
    -----------
    adata1, adata2 : AnnData
        åˆ†æå¯¾è±¡ã®2ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã®AnnDataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    groupby : str
        ç´°èƒã‚¿ã‚¤ãƒ—æƒ…å ±ã‚’å«ã‚€obsã‚«ãƒ©ãƒ å
    db : dict
        CellChatDBãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒª
    cell_types : list
        ç´°èƒã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆ
    n_perm : int
        ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100)
    n_jobs : int
        ä½¿ç”¨ã™ã‚‹CPUã‚³ã‚¢æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 48 - 64ã‚³ã‚¢ã®75%)
    use_gpu : bool
        GPUä½¿ç”¨æœ‰åŠ¹åŒ– (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: True)
    gpu_precision : bool
        å¸¸ã«Trueï¼ˆfloat64å›ºå®šï¼‰
    hybrid_mode : bool
        GPUã¨CPUã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹åŒ– (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: True)
    gpu_ratio : float
        ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰ã§GPUã«å‰²ã‚Šå½“ã¦ã‚‹ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®æ¯”ç‡ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.3)
    optimize_memory : bool
        ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã®æœ‰åŠ¹åŒ– (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: True)
    **kwargs
        CellChatåˆ†æã®ãã®ä»–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    
    Returns:
    --------
    dict
        ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è§£æçµæœ
    """
    # rpy2ã®è­¦å‘Šå¯¾ç­–ã‚’é©ç”¨
    setup_rpy2_warning_fix()
    
    # é–‹å§‹æ™‚é–“è¨˜éŒ²
    global_start_time = time.time()
    
    # 64ã‚³ã‚¢ç’°å¢ƒå‘ã‘NumPyã‚¹ãƒ¬ãƒƒãƒ‰åˆ¶é™
    limit_numpy_threads()
    
    # GPUã®åˆæœŸåŒ–ã¨ç²¾åº¦è¨­å®š
    # ç²¾åº¦ã‚’float64ã«å›ºå®š
    gpu_precision_bool = gpu_precision  # float64ã‚’ä½¿ç”¨
    use_gpu, has_gpu, cp, gpu_dtype = init_gpu(use_gpu, gpu_precision_bool)
    
    # GPUã®çŠ¶æ…‹ãƒ­ã‚®ãƒ³ã‚°
    if has_gpu and use_gpu:
        gpu_info = get_gpu_memory_info()
        if gpu_info:
            st.write(f"GPUæƒ…å ±: {gpu_info['free_gb']:.2f}GB / {gpu_info['total_gb']:.2f}GB åˆ©ç”¨å¯èƒ½")
            st.write(f"ç²¾åº¦: float64 (å›ºå®š)")
            
            # RTX3070ã®ã‚ˆã†ãª8GB GPUã§è­¦å‘Š
            if gpu_info['total_gb'] < 10:
                st.warning("æ³¨æ„: 8GB GPUã§float64ç²¾åº¦ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŸã‚ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¢—åŠ ã—ã¾ã™ã€‚"
                          "ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å ´åˆã¯ã€n_permã‚’æ¸›ã‚‰ã™ã‹ã€"
                          "gpu_ratioå€¤ã‚’ã•ã‚‰ã«ä¸‹ã’ã¦ãã ã•ã„ã€‚")
    
    # 2ã¤ã®AnnDataã‚’çµåˆã—ã€ã‚°ãƒ«ãƒ¼ãƒ—ãƒ©ãƒ™ãƒ«ã‚’ä½œæˆ
    adata_combined = adata1.concatenate(adata2)
    group_labels = np.zeros(adata_combined.n_obs)
    group_labels[:adata1.n_obs] = 0  # Group 1
    group_labels[adata1.n_obs:] = 1  # Group 2
    
    # å®Ÿéš›ã®è§£æï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ï¼‰ã®å®Ÿè¡Œ
    st.write("ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®å®Ÿéš›ã®å·®ç•°ã‚’è¨ˆç®—ä¸­...")
    
    # GPUè¨­å®šã®æœ€é©åŒ–
    kwargs_optimized = kwargs.copy()
    if optimize_memory:
        kwargs_optimized['optimize_memory'] = True
    
    with st.spinner("Group 1ã®CellChatåˆ†æå®Ÿè¡Œä¸­"):
        result1 = cellchat_minimal_analysis_optimized(
            adata1, groupby=groupby, db=db, 
            use_gpu=use_gpu, gpu_precision=gpu_precision, 
            validate_gpu_results=False, **kwargs_optimized
        )
        
        # GPUä½¿ç”¨å¾Œã®ãƒ¡ãƒ¢ãƒªè§£æ”¾
        if has_gpu and use_gpu:
            cp.get_default_memory_pool().free_all_blocks()
            gc.collect()
    
    with st.spinner("Group 2ã®CellChatåˆ†æå®Ÿè¡Œä¸­"):
        result2 = cellchat_minimal_analysis_optimized(
            adata2, groupby=groupby, db=db, 
            use_gpu=use_gpu, gpu_precision=gpu_precision, 
            validate_gpu_results=False, **kwargs_optimized
        )
        
        # GPUä½¿ç”¨å¾Œã®ãƒ¡ãƒ¢ãƒªè§£æ”¾
        if has_gpu and use_gpu:
            cp.get_default_memory_pool().free_all_blocks()
            gc.collect()
    
    actual_diff = calculate_actual_diff(result1, result2, cell_types)
    
    # GPUãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ãƒ­ã‚®ãƒ³ã‚°
    if has_gpu and use_gpu:
        gpu_info = get_gpu_memory_info()
        if gpu_info:
            st.write(f"åˆæœŸåˆ†æå¾Œã®GPUãƒ¡ãƒ¢ãƒª: {gpu_info['used_gb']:.2f}GBä½¿ç”¨ä¸­, {gpu_info['free_gb']:.2f}GBåˆ©ç”¨å¯èƒ½")
    
    # ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆæœ€é©åŒ–
    if use_gpu and has_gpu:
        try:
            # ã‚ˆã‚ŠåŠ¹ç‡çš„ãªãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
            permutation = np.zeros((adata_combined.n_obs, n_perm), dtype=np.int32)
            
            # ãƒãƒƒãƒã‚µã‚¤ã‚ºè¨ˆç®— - å¤§é‡ã®ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¯ãƒãƒ£ãƒ³ã‚¯ã§å‡¦ç†
            chunk_size = min(50, n_perm)
            
            for chunk_start in range(0, n_perm, chunk_size):
                chunk_end = min(chunk_start + chunk_size, n_perm)
                chunk_count = chunk_end - chunk_start
                
                # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã®GPUå‡¦ç†
                cp.random.seed(12345 + chunk_start)  # å†ç¾æ€§ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰è¨­å®š
                try:
                    perm_chunk_gpu = cp.stack([cp.random.permutation(adata_combined.n_obs) 
                                              for _ in range(chunk_count)], axis=1)
                    permutation[:, chunk_start:chunk_end] = cp.asnumpy(perm_chunk_gpu)
                    del perm_chunk_gpu
                except Exception as e:
                    st.warning(f"GPUãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}, CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                    # CPUã§ã®ç”Ÿæˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    for i in range(chunk_count):
                        np.random.seed(12345 + chunk_start + i)
                        permutation[:, chunk_start + i] = np.random.permutation(adata_combined.n_obs)
                
                # å„ãƒãƒ£ãƒ³ã‚¯å¾Œã®GPUãƒ¡ãƒ¢ãƒªè§£æ”¾
                cp.get_default_memory_pool().free_all_blocks()
        except Exception as e:
            st.warning(f"GPUãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}, CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            # CPUç”Ÿæˆã«å®Œå…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            permutation = np.array([np.random.permutation(adata_combined.n_obs) for _ in range(n_perm)]).T
    else:
        # CPUã§ã®ç”Ÿæˆ
        permutation = np.array([np.random.permutation(adata_combined.n_obs) for _ in range(n_perm)]).T
    
    # ãƒãƒƒãƒå‡¦ç†ã®è¨­å®š - 64ã‚³ã‚¢å‘ã‘æœ€é©åŒ–
    batch_size = determine_optimal_batch_size_64core(
        n_perm, n_jobs, 
        adata_combined.n_obs, adata_combined.n_vars,
        has_gpu=has_gpu, 
        gpu_mem=cp.cuda.Device().mem_info[0] if has_gpu else 0
    )
    
    # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´
    if adata_combined.n_obs > 10000 and adata_combined.n_vars > 10000:
        # 12,000éºä¼å­ x 15,000ç´°èƒã®å ´åˆã¯ç‰¹ã«æ…é‡ã«
        original_batch_size = batch_size
        # 64ã‚³ã‚¢ç’°å¢ƒã§ã¯ã€ã‚ˆã‚Šå¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¨±å®¹
        max_batch_size = 30  # 64ã‚³ã‚¢ç’°å¢ƒå‘ã‘æœ€å¤§ãƒãƒƒãƒã‚µã‚¤ã‚º
        if batch_size > max_batch_size:
            batch_size = max_batch_size
            st.write(f"âš ï¸ å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¤œå‡º: ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’{original_batch_size}ã‹ã‚‰{batch_size}ã«èª¿æ•´ã—ã¾ã—ãŸ")
    
    num_batches = (n_perm + batch_size - 1) // batch_size
    st.write(f"{n_jobs}ã‚³ã‚¢ã‚’ä½¿ç”¨ã—ã¦{n_perm}ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’{num_batches}ãƒãƒƒãƒã§å®Ÿè¡Œ...")
    st.write(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: 1ãƒãƒƒãƒã‚ãŸã‚Š{batch_size}ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³")
        
    # å®Ÿè¡Œæ™‚é–“ã®è¦‹ç©ã‚‚ã‚Š
    estimated_time_per_perm = (adata_combined.n_obs * adata_combined.n_vars) / 1e9  # ç§’å˜ä½ã®ç²—ã„è¦‹ç©ã‚‚ã‚Š
    total_est_time = estimated_time_per_perm * n_perm / n_jobs * 1.2  # 64ã‚³ã‚¢å‘ã‘ã«ä¿‚æ•°èª¿æ•´
    st.write(f"æ¨å®šå®Ÿè¡Œæ™‚é–“: ç´„{total_est_time/60:.1f}åˆ† (ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«åŸºã¥ãç²—ã„è¦‹ç©ã‚‚ã‚Š)")
    
    # é€²æ—è¡¨ç¤º
    batch_progress_placeholder = st.empty()
    overall_progress_bar = st.progress(0)
    batch_progress_bar = st.progress(0)
    
    permutation_results = []
    valid_results_count = 0
    start_time = time.time()
    
    # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰ã®GPUå‰²ã‚Šå½“ã¦æœ€é©åŒ–
    if hybrid_mode:
        # 64ã‚³ã‚¢ç’°å¢ƒã§ã¯GPUæ¯”ç‡ã‚’ã‚ˆã‚Šæ§ãˆã‚ã«è¨­å®š
        actual_gpu_ratio = min(gpu_ratio, 0.2) if has_gpu and gpu_info and gpu_info['total_gb'] < 10 else gpu_ratio
        
        # GPUä½¿ç”¨å‰²åˆã«åŸºã¥ã„ã¦GPUç”¨ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é¸æŠ
        n_gpu_perms = int(n_perm * actual_gpu_ratio)
        gpu_indices = set(np.random.choice(range(n_perm), size=n_gpu_perms, replace=False))
        st.write(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰: GPUã§{n_gpu_perms}ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ({actual_gpu_ratio*100:.0f}%), "
                f"CPUã§{n_perm - n_gpu_perms}ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ å®Ÿè¡Œ")
        st.write("æ³¨æ„: float64ç²¾åº¦ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŸã‚ã€GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¢—åŠ ã—ã¾ã™")
    else:
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰ãŒç„¡åŠ¹ã®å ´åˆã€use_gpuã®è¨­å®šã«åŸºã¥ã
        gpu_indices = set(range(n_perm)) if use_gpu else set()
    
    # å„ãƒãƒƒãƒã®å‡¦ç†
    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min((batch_idx + 1) * batch_size, n_perm)
        batch_count = end_idx - start_idx
        
        batch_progress_placeholder.text(f"ãƒãƒƒãƒ {batch_idx+1}/{num_batches} å®Ÿè¡Œä¸­: ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ {start_idx} ã‹ã‚‰ {end_idx-1}")
        batch_start_time = time.time()
        
        # ãƒãƒƒãƒå‰ã®GPUãƒ¡ãƒ¢ãƒªè§£æ”¾
        if has_gpu and use_gpu:
            cp.get_default_memory_pool().free_all_blocks()
            gpu_info = get_gpu_memory_info()
            if gpu_info:
                print(f"ãƒãƒƒãƒ {batch_idx+1} å‰ã®GPUãƒ¡ãƒ¢ãƒª: {gpu_info['free_gb']:.2f}GBåˆ©ç”¨å¯èƒ½")
        
        # é€šå¸¸ã®ä¸¦åˆ—å‡¦ç†ï¼ˆNUMAæœ€é©åŒ–ãªã—ï¼‰
        batch_results = Parallel(n_jobs=n_jobs, verbose=10)(
            delayed(run_permutation_batch_gpu_64core)(
                batch_idx, start_idx + i, min(start_idx + i + 1, end_idx),
                adata_combined, group_labels, groupby, db, cell_types,
                adata1.n_obs, permutation,
                use_gpu=use_gpu and ((start_idx + i) in gpu_indices if hybrid_mode else True),
                gpu_precision=gpu_precision,  # å¸¸ã«float64ã‚’ä½¿ç”¨
                optimize_memory=optimize_memory,
                kwargs=kwargs_optimized
            ) for i in range(batch_count)
        )
        
        # çµæœçµ±åˆ
        flattened_results = [result for sublist in batch_results for result in sublist if result is not None]
        valid_results_count += len(flattened_results)
        permutation_results.extend(flattened_results)
        
        # é€²æ—æ›´æ–°
        overall_progress_bar.progress((batch_idx + 1) / num_batches)
        batch_progress_bar.progress(1.0)
        batch_elapsed_time = time.time() - batch_start_time
        batch_progress_placeholder.text(
            f"ãƒãƒƒãƒ {batch_idx+1}/{num_batches} å®Œäº†: {len(flattened_results)}/{batch_count} æœ‰åŠ¹ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ in {batch_elapsed_time:.1f} ç§’."
        )
        
        # ãƒãƒƒãƒã”ã¨ã®å®Ÿè¡Œé€Ÿåº¦æƒ…å ±
        perms_per_second = len(flattened_results) / max(0.1, batch_elapsed_time)
        remaining_perms = n_perm - (start_idx + batch_count)
        est_remaining_time = remaining_perms / perms_per_second if perms_per_second > 0 else 0
        
        st.write(f"å®Ÿè¡Œé€Ÿåº¦: {perms_per_second:.2f}ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³/ç§’ "
                f"ï¼ˆæ®‹ã‚Šæ¨å®šæ™‚é–“: {est_remaining_time/60:.1f}åˆ†ï¼‰")
        
        # ãƒãƒƒãƒå¾Œã®GPUãƒ¡ãƒ¢ãƒªè§£æ”¾ã¨çŠ¶æ…‹ãƒ­ã‚°
        if has_gpu and use_gpu:
            cp.get_default_memory_pool().free_all_blocks()
            gpu_info = get_gpu_memory_info()
            if gpu_info:
                print(f"ãƒãƒƒãƒ {batch_idx+1} å¾Œã®GPUãƒ¡ãƒ¢ãƒª: {gpu_info['free_gb']:.2f}GBåˆ©ç”¨å¯èƒ½")
        
        # UIã‚’æ›´æ–°ã•ã›ã‚‹ãŸã‚ã®å°ã•ãªé…å»¶
        time.sleep(0.1)
        batch_progress_bar.progress(0.0)
        
        # ãƒãƒƒãƒé–“ã§å¼·åˆ¶çš„ã«ãƒ¡ãƒ¢ãƒªè§£æ”¾
        gc.collect()
    
    elapsed_time = time.time() - start_time
    overall_progress_bar.progress(1.0)
    st.success(f"åˆè¨ˆ{n_perm}ä¸­{valid_results_count}æœ‰åŠ¹ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’{elapsed_time:.1f}ç§’ã§å®Œäº†")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã®è¡¨ç¤º
    perms_per_second_total = valid_results_count / max(0.1, elapsed_time)
    perms_per_core_hour = perms_per_second_total * 3600 / n_jobs
    st.write(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ:")
    st.write(f"- å¹³å‡å‡¦ç†é€Ÿåº¦: {perms_per_second_total:.2f}ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³/ç§’")
    st.write(f"- ã‚³ã‚¢ã‚ãŸã‚Šã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {perms_per_core_hour:.1f}ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³/ã‚³ã‚¢æ™‚é–“")
    st.write(f"- ç·å®Ÿè¡Œæ™‚é–“: {elapsed_time/60:.1f}åˆ†")
    
    # ç·å®Ÿè¡Œæ™‚é–“ã‚’è¡¨ç¤º
    total_time = time.time() - global_start_time
    st.write(f"ç·è§£ææ™‚é–“: {total_time/60:.1f}åˆ†")
    
    # æ—¢å­˜ã®på€¤è¨ˆç®—ã¨å¤šé‡æ¤œå®šè£œæ­£
    p_values = calculate_p_values(actual_diff, permutation_results)
    adjusted_p_values = adjust_p_values(p_values)
    
    return {
        "actual_diff": actual_diff,
        "permutation_results": permutation_results,
        "p_values": p_values,
        "adjusted_p_values": adjusted_p_values,
        "result1": result1,
        "result2": result2,
        "performance_stats": {
            "total_time": total_time,
            "permutation_time": elapsed_time,
            "perms_per_second": perms_per_second_total,
            "perms_per_core_hour": perms_per_core_hour,
            "valid_results": valid_results_count,
            "total_perms": n_perm
        }
    }

# ãƒãƒƒãƒã”ã¨ã®ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è§£æã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°ï¼ˆ64ã‚³ã‚¢ã€NUMAç„¡ã—å‘ã‘ï¼‰
def run_permutation_batch_gpu_64core(batch_idx, start_idx, end_idx, adata_combined, group_labels, 
                                    groupby, db, cell_types, n1, permutation, use_gpu=False, 
                                    gpu_precision=True, optimize_memory=True, kwargs=None):
    """64ã‚³ã‚¢ç’°å¢ƒã«æœ€é©åŒ–ã•ã‚ŒãŸãƒãƒƒãƒãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å‡¦ç†ï¼ˆNUMAæœ€é©åŒ–ãªã—ï¼‰"""
    import os
    import gc
    import time
    import traceback
    import psutil
    
    # rpy2ã®è­¦å‘Šå¯¾ç­–ã‚’é©ç”¨
    setup_rpy2_warning_fix()
    
    start_time = time.time()
    print(f"ãƒãƒƒãƒ {batch_idx} é–‹å§‹: ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ {start_idx} ã‹ã‚‰ {end_idx-1}")
    batch_results = []
    
    # ãƒ—ãƒ­ã‚»ã‚¹IDå–å¾—
    pid = os.getpid()
    
    # 64ã‚³ã‚¢ç’°å¢ƒå‘ã‘ã®NumPyã‚¹ãƒ¬ãƒƒãƒ‰åˆ¶é™
    limit_numpy_threads()
    
    # GPUè¨­å®š
    use_gpu, has_gpu, cp, gpu_dtype = init_gpu(use_gpu, gpu_precision)
    if kwargs is None:
        kwargs = {}
    
    # GPUè³‡æºã®å†ç¢ºèª (å¤šãƒãƒƒãƒå®Ÿè¡Œæ™‚ã«GPUãƒ¡ãƒ¢ãƒªãŒè¶³ã‚Šãªã„çŠ¶æ³ã‚’æ¤œå‡º)
    if has_gpu and use_gpu:
        gpu_info = get_gpu_memory_info()
        if gpu_info and gpu_info['free_gb'] < 1.5:  # 1.5GBæœªæº€ã®ç©ºããƒ¡ãƒ¢ãƒªã—ã‹ãªã„å ´åˆ
            print(f"âš ï¸ è­¦å‘Š: GPUãƒ¡ãƒ¢ãƒªæ®‹ã‚Š{gpu_info['free_gb']:.2f}GBã€‚CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
            use_gpu = False
        elif gpu_info:
            print(f"GPUãƒ¡ãƒ¢ãƒª: ãƒãƒƒãƒ {batch_idx} ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ {start_idx} é–‹å§‹æ™‚ {gpu_info['free_gb']:.2f}GBåˆ©ç”¨å¯èƒ½")
    
    for i in range(start_idx, end_idx):
        perm_start_time = time.time()
        error_occurred = False
        
        try:
            print(f"[ãƒãƒƒãƒ {batch_idx}, ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ {i}] ãƒ—ãƒ­ã‚»ã‚¹ {pid} ã§é–‹å§‹")
            
            # äº‹å‰ã«ç”Ÿæˆã•ã‚ŒãŸãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨
            permuted_indices = permutation[:, i]
            permuted_labels = group_labels[permuted_indices]
            mask1 = permuted_labels == 0
            mask2 = permuted_labels == 1
            
            # ãƒ‡ãƒ¼ã‚¿ãƒãƒ©ãƒ³ã‚¹æ¤œè¨¼
            group1_count = np.sum(mask1)
            group2_count = np.sum(mask2)
            if group1_count < 10 or group2_count < 10:
                print(f"âš ï¸ è­¦å‘Š: ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ {i} ã§ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚ºãŒæ¥µç«¯ã«å°ã•ã„: G1={group1_count}, G2={group2_count}")
                if group1_count < 5 or group2_count < 5:
                    print(f"âš ï¸ ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ {i} ã‚’ã‚¹ã‚­ãƒƒãƒ—: ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚ºãŒå°ã•ã™ãã¾ã™")
                    continue
            
            # shallow copyã‚’ä½¿ç”¨ã—ã¦ãƒ¡ãƒ¢ãƒªä½¿ç”¨ã‚’æœ€é©åŒ–
            adata_perm1 = adata_combined[mask1].copy()
            adata_perm2 = adata_combined[mask2].copy()
            
            # GPUã‹CPUå‡¦ç†ã®é¸æŠ
            try:
                if use_gpu and has_gpu:
                    # GPUå‡¦ç†å‰ã®ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
                    cp.get_default_memory_pool().free_all_blocks()
                    
                    # GPUå‡¦ç† - ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¿½åŠ 
                    perm_result1 = cellchat_minimal_analysis_optimized(
                        adata_perm1, groupby=groupby, db=db,
                        use_gpu=True, gpu_precision=gpu_precision,
                        optimize_memory=optimize_memory, **kwargs
                    )
                    
                    # ä¸­é–“GPUãƒ¡ãƒ¢ãƒªè§£æ”¾
                    cp.get_default_memory_pool().free_all_blocks()
                    gc.collect()
                    
                    # å®Ÿè¡Œæ™‚é–“ãŒé•·ã™ãã‚‹å ´åˆã¯æ®‹ã‚Šã‚’CPUã«åˆ‡ã‚Šæ›¿ãˆ
                    analysis1_time = time.time() - perm_start_time
                    
                    if analysis1_time > 120 and has_gpu:  # 2åˆ†ä»¥ä¸Šã‹ã‹ã‚‹å ´åˆ
                        print(f"âš ï¸ è­¦å‘Š: GPUè§£æã«{analysis1_time:.1f}ç§’ã‹ã‹ã‚Šã¾ã—ãŸã€‚é‡ã„ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚’æ¤œå‡º")
                    
                    # GPUãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
                    gpu_info = get_gpu_memory_info()
                    if gpu_info and gpu_info['free_gb'] < 1.5:  # æ®‹ã‚Šãƒ¡ãƒ¢ãƒªãŒ1.5GBæœªæº€
                        print(f"âš ï¸ è­¦å‘Š: GPUè§£æ1å¾Œã®ãƒ¡ãƒ¢ãƒªæ®‹é‡ãŒå°‘ãªã„ ({gpu_info['free_gb']:.2f}GB)ã€‚CPUã«åˆ‡ã‚Šæ›¿ãˆã¾ã™")
                        # 2ç•ªç›®ã®è§£æã¯CPUã§å®Ÿè¡Œ
                        perm_result2 = cellchat_minimal_analysis_optimized(adata_perm2, groupby=groupby, db=db, **kwargs)
                    else:
                        # 2ç•ªç›®ã®è§£æã‚‚GPUã§å®Ÿè¡Œ
                        perm_result2 = cellchat_minimal_analysis_optimized(
                            adata_perm2, groupby=groupby, db=db,
                            use_gpu=True, gpu_precision=gpu_precision,
                            optimize_memory=optimize_memory, **kwargs
                        )
                else:
                    # CPUå‡¦ç†
                    perm_result1 = cellchat_minimal_analysis_optimized(adata_perm1, groupby=groupby, db=db, **kwargs)
                    perm_result2 = cellchat_minimal_analysis_optimized(adata_perm2, groupby=groupby, db=db, **kwargs)
            except Exception as e:
                print(f"[ãƒãƒƒãƒ {batch_idx}, ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ {i}] è§£æã‚¨ãƒ©ãƒ¼: {str(e)}, CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                print(traceback.format_exc())
                # CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                perm_result1 = cellchat_minimal_analysis_optimized(adata_perm1, groupby=groupby, db=db, **kwargs)
                perm_result2 = cellchat_minimal_analysis_optimized(adata_perm2, groupby=groupby, db=db, **kwargs)
            
            print(f"[ãƒãƒƒãƒ {batch_idx}, ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ {i}] ãƒ—ãƒ­ã‚»ã‚¹ {pid} ã§å·®ç•°è¨ˆç®—ä¸­")
            perm_diff = calculate_actual_diff(perm_result1, perm_result2, cell_types)
            
            perm_time = time.time() - perm_start_time
            print(f"[ãƒãƒƒãƒ {batch_idx}, ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ {i}] ãƒ—ãƒ­ã‚»ã‚¹ {pid} ã§å®Œäº† ({perm_time:.1f}ç§’)")
            batch_results.append(perm_diff)
            
            # ãƒ¡ãƒ¢ãƒªã®æ˜ç¤ºçš„è§£æ”¾
            del adata_perm1, adata_perm2, perm_result1, perm_result2
            
            # GPUä½¿ç”¨å¾Œã®ãƒ¡ãƒ¢ãƒªè§£æ”¾
            if use_gpu and has_gpu:
                cp.get_default_memory_pool().free_all_blocks()
                
                # å®šæœŸçš„ãªGPUãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ãƒ­ã‚®ãƒ³ã‚°ï¼ˆ64ã‚³ã‚¢ç’°å¢ƒã§ã¯ã‚ˆã‚Šé‡è¦ï¼‰
                if optimize_memory:
                    gpu_info = get_gpu_memory_info()
                    if gpu_info:
                        print(f"è§£æå¾ŒGPUãƒ¡ãƒ¢ãƒª ({i}): {gpu_info['free_gb']:.2f}GBåˆ©ç”¨å¯èƒ½")
            
            # å®šæœŸçš„ãªã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆ64ã‚³ã‚¢ç’°å¢ƒã§ã¯é‡è¦ï¼‰
            if i % 2 == 0:  # ã‚ˆã‚Šé »ç¹ã«å®Ÿè¡Œ
                gc.collect()
                
        except Exception as e:
            error_occurred = True
            print(f"[ãƒãƒƒãƒ {batch_idx}, ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ {i}] ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)}")
            print(traceback.format_exc())
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        if not error_occurred:
            perm_time = time.time() - perm_start_time
            time_stats = f"{perm_time:.1f}ç§’"
            
            # é•·æ™‚é–“å®Ÿè¡Œã®è­¦å‘Š
            if perm_time > 300:  # 5åˆ†ä»¥ä¸Š
                time_stats += " âš ï¸ é•·æ™‚é–“å®Ÿè¡Œ!"
            
            print(f"[ãƒãƒƒãƒ {batch_idx}, ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ {i}] å®Ÿè¡Œæ™‚é–“: {time_stats}")
            
            # 64ã‚³ã‚¢ç’°å¢ƒã§ã®ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡º
            try:
                # ç¾åœ¨ã®ãƒ—ãƒ­ã‚»ã‚¹ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
                memory_info = psutil.Process(pid).memory_info()
                memory_usage_gb = memory_info.rss / 1e9
                
                if memory_usage_gb > 8:  # 8GBä»¥ä¸Šä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯è­¦å‘Š
                    print(f"âš ï¸ è­¦å‘Š: ãƒ—ãƒ­ã‚»ã‚¹ {pid} ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒé«˜ã„: {memory_usage_gb:.2f}GB")
                    # å¼·åˆ¶çš„ã«ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
                    gc.collect()
                
            except:
                pass
    
    # æœ€çµ‚çš„ãªãƒ¡ãƒ¢ãƒªè§£æ”¾
    if use_gpu and has_gpu:
        cp.get_default_memory_pool().free_all_blocks()
    gc.collect()
    
    batch_time = time.time() - start_time
    print(f"ãƒãƒƒãƒ {batch_idx} å®Œäº†: {len(batch_results)}/{end_idx-start_idx} æˆåŠŸ, åˆè¨ˆ{batch_time:.1f}ç§’")
    
    return batch_results

# GPUåˆæœŸåŒ–ç”¨é–¢æ•°ï¼šgpu_precision ãŒ True ãªã‚‰ float64ã€False ãªã‚‰ float32 ã¨ã™ã‚‹
def init_gpu(use_gpu, gpu_precision):
    has_gpu = False
    cp = None
    gpu_dtype = None
    if use_gpu:
        try:
            import cupy as cp
            has_gpu = True
            # gpu_precision ãŒ True ãªã‚‰ float64ã€False ãªã‚‰ float32
            gpu_dtype = cp.float64 if gpu_precision else cp.float32
            st.write("GPU available for permutation testing")
        except ImportError:
            st.write("GPU requested but cupy not available, falling back to CPU")
            use_gpu = False
    return use_gpu, has_gpu, cp, gpu_dtype


# ãƒãƒƒãƒã”ã¨ã«ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è§£æã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°
def run_permutation_batch_gpu(batch_idx, start_idx, end_idx, adata_combined, group_labels, 
                              groupby, db, cell_types, n1, permutation, use_gpu=False, 
                              gpu_precision=True, kwargs=None):
    print(f"Starting batch {batch_idx}: permutations {start_idx} to {end_idx-1}")
    batch_results = []

    use_gpu, has_gpu, cp, gpu_dtype = init_gpu(use_gpu, gpu_precision)
    if kwargs is None:
        kwargs = {}

    for i in range(start_idx, end_idx):
        try:
            pid = os.getpid()
            print(f"[Batch {batch_idx}, Permutation {i}] Starting in process {pid}")
            # äº‹å‰ã«ç”Ÿæˆã•ã‚ŒãŸãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨
            permuted_indices = permutation[:, i]
            permuted_labels = group_labels[permuted_indices]
            mask1 = permuted_labels == 0
            mask2 = permuted_labels == 1
            adata_perm1 = adata_combined[mask1].copy()
            adata_perm2 = adata_combined[mask2].copy()

            # GPUãŒåˆ©ç”¨å¯èƒ½ãªã‚‰ GPU æœ€é©åŒ–ç‰ˆè§£æé–¢æ•°ã‚’åˆ©ç”¨
            if use_gpu and has_gpu:
                perm_result1 = cellchat_minimal_analysis_optimized(
                    adata_perm1, groupby=groupby, db=db,
                    use_gpu=True, gpu_precision=gpu_precision, **kwargs
                )
                perm_result2 = cellchat_minimal_analysis_optimized(
                    adata_perm2, groupby=groupby, db=db,
                    use_gpu=True, gpu_precision=gpu_precision, **kwargs
                )
            else:
                perm_result1 = cellchat_minimal_analysis_optimized(adata_perm1, groupby=groupby,
                    db=db,use_gpu=False, gpu_precision=gpu_precision, **kwargs)
                perm_result2 = cellchat_minimal_analysis_optimized(adata_perm2, groupby=groupby, 
                    db=db,use_gpu=False, gpu_precision=gpu_precision,**kwargs)

            print(f"[Batch {batch_idx}, Permutation {i}] Calculating differences in process {pid}")
            perm_diff = calculate_actual_diff(perm_result1, perm_result2, cell_types)
            print(f"[Batch {batch_idx}, Permutation {i}] Completed in process {pid}")
            batch_results.append(perm_diff)

            if use_gpu and has_gpu:
                cp.get_default_memory_pool().free_all_blocks()

        except Exception as e:
            print(f"[Batch {batch_idx}, Permutation {i}] Error: {str(e)}")
            import traceback
            print(traceback.format_exc())

    if use_gpu and has_gpu:
        cp.get_default_memory_pool().free_all_blocks()

    return batch_results

def cellchat_minimal_analysis_optimized(
    adata,
    groupby,
    db,
    use_layer=None,
    min_cells=10,
    expr_prop=0.1,
    pseudocount=1.0,
    trim_threshold=0.05,
    k=0.5,
    n=1,
    type_mean="triMean",
    raw_use=True,
    population_size=False,
    nboot=100,
    seed=12345,
    n_jobs=8,
    key_added="cellchat_res",
    trim=0.1,
    apply_pval_filter=True,
    features=None,
    use_gpu=True,
    gpu_precision=True,
    validate_gpu_results=False,
    gpu_memory_limit=0.8,
    optimize_memory=True
):
    """
    GPU-optimized minimal version of CellChat algorithm for permutation testing.
    
    Parameters
    ----------
    adata : AnnData
        AnnData object
    groupby : str
        Column name in .obs containing cell type/cluster information
    db : dict
        CellChatDB dictionary
    use_layer : str, optional
        Data layer to use
    [ä»–ã®æ—¢å­˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿]
    use_gpu : bool, optional
        Whether to use GPU acceleration. Default is True
    gpu_precision : str, optional
        Precision for GPU calculations ('float64' or 'float32'). Default is 'float64'
    validate_gpu_results : bool, optional
        Whether to validate GPU results against CPU. Default is False
    gpu_memory_limit : float, optional
        GPU memory usage limit (0-1). Default is 0.8
    optimize_memory : bool, optional
        Whether to optimize memory usage. Default is True
        
    Returns
    -------
    dict
        Minimal cell-cell communication analysis results
    """
    
    # Initialize logger
    logger = logging.getLogger("CellChat-Minimal-Optimized")
    logger.setLevel(logging.INFO)
    
    # GPUåˆæœŸåŒ–
    has_gpu = False
    cp = None

    if gpu_precision:
        data_type = np.float64  # True = é«˜ç²¾åº¦
    else:
        data_type = np.float32  # False = ä½ç²¾åº¦
    
    if use_gpu:
        try:
            import cupy as cp
            has_gpu = True
            # GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’åˆ¶é™
            if gpu_memory_limit > 0 and gpu_memory_limit <= 1.0:
                mempool = cp.get_default_memory_pool()
                mempool.set_limit(cp.cuda.Device().mem_info[1] * gpu_memory_limit)
            # ç²¾åº¦ã®è¨­å®š
            gpu_dtype = cp.float64 if gpu_precision else cp.float32
            logger.info(f"GPU acceleration enabled using {gpu_precision} precision")
        except ImportError:
            logger.info("GPU acceleration requested but cupy not available, falling back to CPU")
            use_gpu = False
    
    try:
        # ä¹±æ•°ã®ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®š
        np.random.seed(seed)
        if has_gpu and use_gpu:
            cp.random.seed(seed)
        
        # GPUç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if has_gpu and use_gpu:
            try:
                from pages.gpu_cellchat_helpers import (
                    compute_hill_outer_gpu,
                    precompute_gene_expressions_gpu,
                    geometricMean_gpu
                )
            except ImportError:
                logger.warning("GPU helper functions not available, falling back to CPU implementations")
        
        # ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
        if features is not None:
            logger.info(f"Using provided gene list: {len(features)} genes")
            features = [f for f in features if f in adata.var_names]
            adata_filtered = adata[:, features].copy()
        else:
            adata_filtered = preprocess_data(adata, groupby, min_cells=min_cells, thresh_pct=expr_prop)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å–å¾—
        resource = db.get('interaction', pd.DataFrame())
        complex_input = db.get('complex', pd.DataFrame())
        cofactor_input = db.get('cofactor', pd.DataFrame())
        
        # åŸºæœ¬çš„ãªæ¤œè¨¼ï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ«ã¨åŒã˜ï¼‰
        if resource.empty:
            raise ValueError("Empty interaction information in CellChatDB.")
        for required_col in ['ligand', 'receptor']:
            if required_col not in resource.columns:
                raise ValueError(f"Required column '{required_col}' not found in resource dataframe")
        
        # ç™ºç¾ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        if use_layer is not None and use_layer in adata_filtered.layers:
            logger.info(f"Using layer '{use_layer}'")
            X = adata_filtered.layers[use_layer]
        else:
            logger.info("Using default X matrix")
            X = adata_filtered.X
        
        # ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—ã‚’å¯†è¡Œåˆ—ã«å¤‰æ›ï¼ˆåŠ¹ç‡çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨GPUã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if scipy.sparse.issparse(X):
            data_size = X.shape[0] * X.shape[1]
            
            # GPUã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€å¤§ãã„ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—ã¯ç›´æ¥GPUã§å¤‰æ›ã‚’è©¦ã¿ã‚‹
            if has_gpu and use_gpu and data_size < cp.cuda.Device().mem_info[0] * 0.3:
                try:
                    import cupyx.scipy.sparse as cusparse
                    logger.info("Converting sparse matrix on GPU")
                    X_gpu = cusparse.csr_matrix(X).astype(data_type)
                    X = cp.asnumpy(X_gpu.toarray())
                    del X_gpu
                    cp.get_default_memory_pool().free_all_blocks()
                except Exception as e:
                    logger.warning(f"GPU sparse conversion failed: {str(e)}, falling back to CPU")
                    # CPUã§ã®åŠ¹ç‡çš„ãªå¤‰æ›
                    if optimize_memory and X.shape[0] > 5000:
                        chunk_size = 5000
                        result = []
                        for i in range(0, X.shape[0], chunk_size):
                            end = min(i + chunk_size, X.shape[0])
                            result.append(X[i:end].toarray().astype(data_type))
                        X = np.vstack(result)
                    else:
                        X = X.toarray().astype(data_type)
            else:
                # CPUã§ã®åŠ¹ç‡çš„ãªå¤‰æ›
                if optimize_memory and X.shape[0] > 5000:
                    chunk_size = 5000
                    result = []
                    for i in range(0, X.shape[0], chunk_size):
                        end = min(i + chunk_size, X.shape[0])
                        result.append(X[i:end].toarray().astype(data_type))
                    X = np.vstack(result)
                else:
                    X = X.toarray().astype(data_type)
        
        # ç´°èƒã‚¿ã‚¤ãƒ—ãƒ©ãƒ™ãƒ«ã®å–å¾—
        cell_labels = adata_filtered.obs[groupby].copy()
        cell_types = np.array(sorted(cell_labels.unique()))
        
        logger.info(f"Number of cell types: {len(cell_types)}")
        if len(cell_types) < 2:
            raise ValueError(f"Only {len(cell_types)} cell type found. At least 2 required.")
        
        # CPUã¨GPUã®ä¸¡æ–¹ã§ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–
        data_use_cpu = X / np.max(X).astype(data_type)
        
        if has_gpu and use_gpu:
            # GPUç‰ˆ
            X_gpu = cp.array(X, dtype=gpu_dtype)
            max_val_gpu = cp.max(X_gpu).astype(gpu_dtype)
            data_use_gpu = X_gpu / max_val_gpu
            
            # æ¤œè¨¼
            if validate_gpu_results:
                # GPUçµæœã¨CPUçµæœã®æ¯”è¼ƒ
                data_use_gpu_cpu = cp.asnumpy(data_use_gpu)
                max_diff = np.max(np.abs(data_use_cpu - data_use_gpu_cpu))
                if max_diff > 1e-5:
                    logger.warning(f"GPU and CPU normalization results differ: {max_diff}. Using CPU.")
                    data_use = data_use_cpu
                    del X_gpu, data_use_gpu
                    cp.get_default_memory_pool().free_all_blocks()
                else:
                    data_use = data_use_gpu
            else:
                data_use = data_use_gpu
        else:
            # CPUç‰ˆã®ã¿
            data_use = data_use_cpu.copy()
        
        nC = data_use.shape[0] if isinstance(data_use, np.ndarray) else data_use.shape[0]
        
        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã®ãŸã‚ã€å…ƒãƒ‡ãƒ¼ã‚¿ã‚’è§£æ”¾
        if optimize_memory:
            del X
            gc.collect()
            if has_gpu and use_gpu:
                if 'X_gpu' in locals():
                    del X_gpu
                cp.get_default_memory_pool().free_all_blocks()
        
        # å¹³å‡é–¢æ•°ã®å®šç¾©
        if type_mean == "triMean":
            # GPUå¯¾å¿œã®trimeean
            def FunMean_gpu(x):
                if has_gpu and use_gpu and isinstance(x, cp.ndarray):
                    x_no_nan = x[~cp.isnan(x)]
                    if len(x_no_nan) == 0:
                        return cp.nan
                    q = cp.quantile(x_no_nan, cp.array([0.25, 0.5, 0.5, 0.75]))
                    return cp.mean(q).astype(gpu_dtype)
                else:
                    x_no_nan = x[~np.isnan(x)]
                    if len(x_no_nan) == 0:
                        return np.nan
                    q = np.quantile(x_no_nan, [0.25, 0.5, 0.5, 0.75], method='linear')
                    return np.mean(q).astype(gpu_dtype)
            
            # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®CPUç‰ˆ
            def FunMean(x):
                x_no_nan = x[~np.isnan(x)]
                if len(x_no_nan) == 0:
                    return np.nan
                q = np.quantile(x_no_nan, [0.25, 0.5, 0.5, 0.75], method='linear')
                return np.mean(q)
        elif type_mean == "truncatedMean":
            from scipy.stats import trim_mean
            def FunMean(x):
                return trim_mean(x, proportiontocut=trim)
            # GPUç‰ˆã‚‚åŒæ§˜ã«å®šç¾©å¯èƒ½
        elif type_mean == "median":
            def FunMean(x):
                return np.median(x)
        else:
            def FunMean(x):
                return np.mean(x)
        
        # ç´°èƒã‚¿ã‚¤ãƒ—ã”ã¨ã®å¹³å‡ç™ºç¾é‡ã®è¨ˆç®—ï¼ˆæœ€é©åŒ–ï¼‰
        # äº‹å‰ã«ç´°èƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—ã—ã¦ç¹°ã‚Šè¿”ã—æ¤œç´¢ã‚’é¿ã‘ã‚‹
        cell_counts = {}
        cell_type_indices = {}
        for cell_type in cell_types:
            indices = np.where(cell_labels == cell_type)[0]
            cell_counts[cell_type] = len(indices)
            cell_type_indices[cell_type] = indices
        
        # æœ‰åŠ¹ãªç´°èƒã‚¿ã‚¤ãƒ—ã‚’ç‰¹å®š
        valid_cell_types = [ct for ct in cell_types if cell_counts.get(ct, 0) >= min_cells]
        if len(valid_cell_types) < 2:
            raise ValueError(f"Less than 2 cell types have at least {min_cells} cells.")
        
        # GPUå¯¾å¿œã®å¹³å‡ç™ºç¾è¨ˆç®—
        data_use_avg_dict = {}
        
        # GPUã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã®åŠ¹ç‡çš„ãªå¹³å‡ç™ºç¾è¨ˆç®—
        if has_gpu and use_gpu and isinstance(data_use, cp.ndarray):
            for cell_type in valid_cell_types:
                indices = cell_type_indices[cell_type]
                indices_gpu = cp.array(indices)
                
                # å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å ´åˆã¯ãƒãƒƒãƒå‡¦ç†
                if optimize_memory and data_use.shape[1] > 10000:
                    gene_chunk_size = 1000
                    n_genes = data_use.shape[1]
                    avg_expr = cp.zeros(n_genes, dtype=gpu_dtype)
                    
                    for i in range(0, n_genes, gene_chunk_size):
                        end = min(i + gene_chunk_size, n_genes)
                        data_subset = data_use[indices_gpu, i:end]
                        temp_result = cp.apply_along_axis(FunMean_gpu, 0, data_subset)
                        avg_expr[i:end] = temp_result
                else:
                    data_subset = data_use[indices_gpu]
                    avg_expr = cp.apply_along_axis(FunMean_gpu, 0, data_subset)
                
                # GPUçµæœã®æ¤œè¨¼
                if validate_gpu_results:
                    cpu_indices = np.array(indices)
                    cpu_data_subset = data_use_cpu[cpu_indices]
                    cpu_avg_expr = np.apply_along_axis(FunMean, 0, cpu_data_subset)
                    
                    # CPUçµæœã¨GPUçµæœã®æ¯”è¼ƒ
                    avg_expr_cpu = cp.asnumpy(avg_expr)
                    max_diff = np.max(np.abs(cpu_avg_expr - avg_expr_cpu))
                    if max_diff > 1e-5:
                        logger.warning(f"GPU and CPU mean calculation differ: {max_diff}. Using CPU.")
                        data_use_avg_dict[cell_type] = cpu_avg_expr
                    else:
                        data_use_avg_dict[cell_type] = cp.asnumpy(avg_expr)
                else:
                    data_use_avg_dict[cell_type] = cp.asnumpy(avg_expr)
        else:
            # CPUç‰ˆã®è¨ˆç®—ï¼ˆæœ€é©åŒ–ï¼‰
            for cell_type in valid_cell_types:
                indices = cell_type_indices[cell_type]
                
                # å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯åˆ†å‰²å‡¦ç†
                if optimize_memory and data_use_cpu.shape[1] > 10000:
                    gene_chunk_size = 1000
                    n_genes = data_use_cpu.shape[1]
                    avg_expr = np.zeros(n_genes)
                    
                    for i in range(0, n_genes, gene_chunk_size):
                        end = min(i + gene_chunk_size, n_genes)
                        data_subset = data_use_cpu[indices, i:end]
                        avg_expr[i:end] = np.apply_along_axis(FunMean, 0, data_subset)
                else:
                    data_subset = data_use_cpu[indices]
                    avg_expr = np.apply_along_axis(FunMean, 0, data_subset)
                
                data_use_avg_dict[cell_type] = avg_expr
        
        # DataFrameã«å¤‰æ›
        data_use_avg_df = pd.DataFrame(data_use_avg_dict, index=adata_filtered.var_names)
        
        # éºä¼å­åã‹ã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        gene_to_index = {gene: i for i, gene in enumerate(adata_filtered.var_names)}
        
        # ãƒªã‚¬ãƒ³ãƒ‰ãƒ»ãƒ¬ã‚»ãƒ—ã‚¿ãƒ¼ç™ºç¾ãƒ¬ãƒ™ãƒ«ã®è¨ˆç®— - ã‚ªãƒªã‚¸ãƒŠãƒ«é–¢æ•°ã‚’ä½¿ç”¨
        dataLavg = computeExpr_LR(resource['ligand'].values, data_use_avg_df, complex_input)
        dataRavg = computeExpr_LR(resource['receptor'].values, data_use_avg_df, complex_input)
        
        # å…±å—å®¹ä½“ã®è¨ˆç®— - ã‚ªãƒªã‚¸ãƒŠãƒ«é–¢æ•°ã‚’ä½¿ç”¨
        dataRavg_co_A_receptor = computeExpr_coreceptor(cofactor_input, data_use_avg_df, resource, "A")
        dataRavg_co_I_receptor = computeExpr_coreceptor(cofactor_input, data_use_avg_df, resource, "I")
        dataRavg = dataRavg * dataRavg_co_A_receptor / dataRavg_co_I_receptor
        
        # é›†å›£ã‚µã‚¤ã‚ºåŠ¹æœãŒæœ‰åŠ¹ãªå ´åˆ - ã‚ªãƒªã‚¸ãƒŠãƒ«ã¨åŒã˜è¨ˆç®—
        if population_size:
            cell_proportions = np.array([np.sum(cell_labels == ct) for ct in valid_cell_types]) / nC
            dataLavg2 = np.tile(cell_proportions, (len(resource), 1))
            dataRavg2 = dataLavg2
        else:
            dataLavg2 = np.ones((len(resource), len(valid_cell_types)))
            dataRavg2 = np.ones((len(resource), len(valid_cell_types)))
        
        # ã‚¢ã‚´ãƒ‹ã‚¹ãƒˆ/ã‚¢ãƒ³ã‚¿ã‚´ãƒ‹ã‚¹ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç‰¹å®š - ã‚ªãƒªã‚¸ãƒŠãƒ«ã¨åŒã˜
        index_agonist = np.where(resource['agonist'].notna() & (resource['agonist'] != ""))[0] if 'agonist' in resource.columns else []
        index_antagonist = np.where(resource['antagonist'].notna() & (resource['antagonist'] != ""))[0] if 'antagonist' in resource.columns else []
        
        # ç½®æ›ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ - GPUå¯¾å¿œ
        permutation = np.zeros((nC, nboot), dtype=np.int32)
        
        if has_gpu and use_gpu:
            # GPUã§ã®ç”Ÿæˆ
            for i in range(nboot):
                perm_gpu = cp.random.permutation(nC)
                permutation[:, i] = cp.asnumpy(perm_gpu)
        else:
            # CPUç‰ˆ
            for i in range(nboot):
                permutation[:, i] = np.random.permutation(nC)
        
        # éºä¼å­ç™ºç¾ã®äº‹å‰è¨ˆç®— - GPUå¯¾å¿œ
        if nboot > 0:
            # GPUã®åˆ©ç”¨å¯èƒ½æ€§ã«å¿œã˜ã¦é©åˆ‡ãªé–¢æ•°ã‚’é¸æŠ
            if has_gpu and use_gpu:
                try:
                    all_gene_expr = precompute_gene_expressions_gpu(
                        data_use_cpu,  # å¸¸ã«CPUãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã™
                        cell_labels, 
                        permutation, 
                        valid_cell_types, 
                        FunMean_gpu if has_gpu and use_gpu else FunMean, 
                        nboot, 
                        n_jobs,
                        gpu_dtype
                    )
                    
                    if validate_gpu_results:
                        # ã‚µãƒ³ãƒ—ãƒ«ã®ä¸€éƒ¨ã§CPUè¨ˆç®—ã¨æ¯”è¼ƒæ¤œè¨¼
                        sample_size = min(5, nboot)
                        cpu_result = precompute_gene_expressions(
                            data_use_cpu,
                            cell_labels,
                            permutation[:, :sample_size],
                            valid_cell_types,
                            FunMean,
                            sample_size,
                            n_jobs
                        )
                        
                        # æœ€åˆã®ã„ãã¤ã‹ã®çµæœã‚’æ¯”è¼ƒ
                        max_diff = np.max(np.abs(cpu_result[:, :, :3] - all_gene_expr[:, :, :3]))
                        if max_diff > 1e-5:
                            logger.warning(f"GPU and CPU precomputation differ: {max_diff}. Using CPU.")
                            all_gene_expr = precompute_gene_expressions(
                                data_use_cpu,
                                cell_labels,
                                permutation,
                                valid_cell_types,
                                FunMean,
                                nboot,
                                n_jobs
                            )
                except Exception as e:
                    logger.warning(f"GPU gene expression precomputation failed: {str(e)}. Using CPU.")
                    all_gene_expr = precompute_gene_expressions(
                        data_use_cpu,
                        cell_labels,
                        permutation,
                        valid_cell_types,
                        FunMean,
                        nboot,
                        n_jobs
                    )
            else:
                # CPUç‰ˆ
                all_gene_expr = precompute_gene_expressions(
                    data_use_cpu,
                    cell_labels,
                    permutation,
                    valid_cell_types,
                    FunMean,
                    nboot,
                    n_jobs
                )
        
        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã®ãŸã‚ã®è§£æ”¾
        if optimize_memory:
            if isinstance(data_use, cp.ndarray):
                del data_use
                cp.get_default_memory_pool().free_all_blocks()
            else:
                del data_use
            # data_use_cpuã¯ä»–ã®è¨ˆç®—ã§ã¾ã å¿…è¦ãªå ´åˆã¯ä¿æŒ
            gc.collect()
        
        # è¤‡åˆä½“ãƒãƒƒãƒ”ãƒ³ã‚°ã®äº‹å‰è¨ˆç®— - çµæœã«å½±éŸ¿ã—ãªã„æœ€é©åŒ–
        complex_mapping = {}
        if not complex_input.empty:
            for complex_name in complex_input.index:
                subunits_cols = [col for col in complex_input.columns if 'subunit' in col]
                subunits = complex_input.loc[complex_name, subunits_cols].dropna().astype(str)
                subunits = [s for s in subunits if s != "" and s in gene_to_index]
                
                if subunits:
                    complex_mapping[complex_name] = [gene_to_index[s] for s in subunits]
        
        # ç¢ºç‡ã¨æœ‰æ„æ€§ã®è¡Œåˆ—ã‚’åˆæœŸåŒ–
        numCluster = len(valid_cell_types)
        nLR = len(resource)
        Prob = np.zeros((numCluster, numCluster, nLR))
        Pval = np.zeros((numCluster, numCluster, nLR))
        
        # ãƒªã‚¬ãƒ³ãƒ‰ãƒ»ãƒ¬ã‚»ãƒ—ã‚¿ãƒ¼éºä¼å­ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹äº‹å‰è¨ˆç®— - çµæœã«å½±éŸ¿ã—ãªã„æœ€é©åŒ–
        ligand_indices = []
        receptor_indices = []
        
        for i in range(nLR):
            ligand = resource['ligand'].iloc[i]
            receptor = resource['receptor'].iloc[i]
            
            # å˜ä¸€éºä¼å­ã‹è¤‡åˆä½“ã‹ã‚’åˆ¤å®š
            if isinstance(ligand, str) and ligand in gene_to_index:
                ligand_indices.append((i, [gene_to_index[ligand]], False))
            elif isinstance(ligand, str) and ligand in complex_mapping:
                ligand_indices.append((i, complex_mapping[ligand], True))
            else:
                ligand_indices.append((i, [], None))
            
            if isinstance(receptor, str) and receptor in gene_to_index:
                receptor_indices.append((i, [gene_to_index[receptor]], False))
            elif isinstance(receptor, str) and receptor in complex_mapping:
                receptor_indices.append((i, complex_mapping[receptor], True))
            else:
                receptor_indices.append((i, [], None))
        
        # å„LRãƒšã‚¢ã‚’å‡¦ç† - GPUæœ€é©åŒ–
        for i in range(nLR):
            # Hillé–¢æ•°ã«ã‚ˆã‚‹ç›¸äº’ä½œç”¨ç¢ºç‡ã®è¨ˆç®— - GPU or CPU
            if has_gpu and use_gpu:
                try:
                    # GPUå®Ÿè£…
                    dataLavg_gpu = cp.array(dataLavg[i, :], dtype=gpu_dtype)
                    dataRavg_gpu = cp.array(dataRavg[i, :], dtype=gpu_dtype)
                    
                    # GPUç”¨Hillé–¢æ•°ã®å‘¼ã³å‡ºã—
                    P1_gpu = compute_hill_outer_gpu(dataLavg_gpu, dataRavg_gpu, k, n)
                    
                    # CPUçµæœã¨æ¯”è¼ƒæ¤œè¨¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                    if validate_gpu_results:
                        P1_cpu = compute_hill_outer(dataLavg[i, :], dataRavg[i, :], k, n)
                        P1_gpu_cpu = cp.asnumpy(P1_gpu)
                        max_diff = np.max(np.abs(P1_cpu - P1_gpu_cpu))
                        
                        if max_diff > 1e-5:
                            logger.warning(f"GPU and CPU Hill function differ: {max_diff}. Using CPU.")
                            P1 = P1_cpu
                        else:
                            P1 = cp.asnumpy(P1_gpu)
                    else:
                        P1 = cp.asnumpy(P1_gpu)
                    
                    # ãƒ¡ãƒ¢ãƒªè§£æ”¾
                    del dataLavg_gpu, dataRavg_gpu, P1_gpu
                    if i % 50 == 0:  # å®šæœŸçš„ã«ãƒ¡ãƒ¢ãƒªè§£æ”¾
                        cp.get_default_memory_pool().free_all_blocks()
                except Exception as e:
                    logger.warning(f"GPU Hill function failed: {str(e)}. Using CPU.")
                    P1 = compute_hill_outer(dataLavg[i, :], dataRavg[i, :], k, n)
            else:
                # CPUå®Ÿè£…
                P1 = compute_hill_outer(dataLavg[i, :], dataRavg[i, :], k, n)
            
            # ã‚¢ã‚´ãƒ‹ã‚¹ãƒˆåŠ¹æœ - ã‚ªãƒªã‚¸ãƒŠãƒ«é–¢æ•°ã‚’ä½¿ç”¨
            P2 = np.ones((numCluster, numCluster))
            if i in index_agonist:
                data_agonist = computeExpr_agonist(data_use_avg_df, resource, cofactor_input, i, k, n)
                P2 = np.outer(data_agonist, data_agonist)
            
            # ã‚¢ãƒ³ã‚¿ã‚´ãƒ‹ã‚¹ãƒˆåŠ¹æœ - ã‚ªãƒªã‚¸ãƒŠãƒ«é–¢æ•°ã‚’ä½¿ç”¨
            P3 = np.ones((numCluster, numCluster))
            if i in index_antagonist:
                data_antagonist = computeExpr_antagonist(data_use_avg_df, resource, cofactor_input, i, k, n)
                P3 = np.outer(data_antagonist, data_antagonist)
            
            # é›†å›£ã‚µã‚¤ã‚ºåŠ¹æœ - ã‚ªãƒªã‚¸ãƒŠãƒ«ã¨åŒã˜è¨ˆç®—
            P4 = np.ones((numCluster, numCluster))
            if population_size:
                P4 = np.outer(dataLavg2[i, :], dataRavg2[i, :])
            
            # æœ€çµ‚ç¢ºç‡ - ã‚ªãƒªã‚¸ãƒŠãƒ«ã¨åŒã˜è¨ˆç®—
            Pnull = P1 * P2 * P3 * P4
            Prob[:, :, i] = Pnull
            
            # ç›¸äº’ä½œç”¨ãŒãªã„å ´åˆã¯på€¤è¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if np.sum(Pnull) == 0 or nboot == 0:
                Pval[:, :, i] = 1
                continue
            
            Pnull_vec = Pnull.flatten()
            
            # ãƒªã‚¬ãƒ³ãƒ‰ãƒ»ãƒ¬ã‚»ãƒ—ã‚¿ãƒ¼æƒ…å ±ã®å–å¾—
            ligand_info = ligand_indices[i]
            receptor_info = receptor_indices[i]
            
            # ç™ºç¾å–å¾—ã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if ligand_info[2] is None or receptor_info[2] is None:
                Pval[:, :, i] = 1
                continue
            
            # ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ç¢ºç‡ã®åˆæœŸåŒ–
            Pboot = np.zeros((numCluster * numCluster, nboot))
            
            # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã®ãŸã‚ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’èª¿æ•´
            batch_size = min(20, nboot)  
            
            # ä¸¦åˆ—å‡¦ç†ã§åŠ¹ç‡çš„ã«è¨ˆç®—
            n_jobs_effective = min(n_jobs, os.cpu_count() or 1, nboot)
            
            # GPUä½¿ç”¨ã‚’åˆ¶å¾¡
            use_gpu_for_batches = has_gpu and use_gpu
            
            # ãƒãƒƒãƒã”ã¨ã®å‡¦ç†
            from joblib import Parallel, delayed
            
            # ç½®æ›è¨ˆç®—ãƒãƒƒãƒé–¢æ•°ã®å®šç¾©
            def compute_permutation_batch(batch_indices, ligand_info, receptor_info,
                                         numCluster, n, k, population_size, cell_labels,
                                         permutation, valid_cell_types, nC, use_gpu_compute=False):
                batch_results = np.zeros((numCluster * numCluster, len(batch_indices)))
                
                for idx, j in enumerate(batch_indices):
                    # ãƒªã‚¬ãƒ³ãƒ‰ç™ºç¾ã®å–å¾—
                    lr_i, ligand_gene_indices, is_ligand_complex = ligand_info
                    
                    if not is_ligand_complex:
                        # å˜ä¸€éºä¼å­
                        if ligand_gene_indices:
                            ligand_idx = ligand_gene_indices[0]
                            dataLavgB = all_gene_expr[ligand_idx, :, j].astype(data_type).reshape(1, -1)
                        else:
                            dataLavgB = np.zeros((1, numCluster))
                    else:
                        # è¤‡åˆä½“ - å¹¾ä½•å¹³å‡è¨ˆç®—
                        expr_values = np.array([all_gene_expr[idx, :, j] for idx in ligand_gene_indices])
                        if len(expr_values) > 0:
                            # å¯¾æ•°å¤‰æ›ã€å¹³å‡ç®—å‡ºã€é€†å¤‰æ›
                            log_values = np.log(expr_values + 1e-10)
                            dataLavgB = np.exp(np.mean(log_values, axis=0)).astype(data_type).reshape(1, -1)
                        else:
                            dataLavgB = np.zeros((1, numCluster))
                    
                    # ãƒ¬ã‚»ãƒ—ã‚¿ãƒ¼ç™ºç¾ã®å–å¾—
                    lr_i, receptor_gene_indices, is_receptor_complex = receptor_info
                    
                    if not is_receptor_complex:
                        if receptor_gene_indices:
                            receptor_idx = receptor_gene_indices[0]
                            dataRavgB = all_gene_expr[receptor_idx, :, j].reshape(1, -1)
                        else:
                            dataRavgB = np.zeros((1, numCluster))
                    else:
                        expr_values = np.array([all_gene_expr[idx, :, j] for idx in receptor_gene_indices])
                        if len(expr_values) > 0:
                            log_values = np.log(expr_values + 1e-10)
                            dataRavgB = np.exp(np.mean(log_values, axis=0)).reshape(1, -1)
                        else:
                            dataRavgB = np.zeros((1, numCluster))
                    
                    # ç›¸äº’ä½œç”¨ç¢ºç‡ã®è¨ˆç®— - GPUä½¿ç”¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³
                    if use_gpu_compute and has_gpu:
                        try:
                            dataLavgB_gpu = cp.array(dataLavgB[0, :], dtype=gpu_dtype)
                            dataRavgB_gpu = cp.array(dataRavgB[0, :], dtype=gpu_dtype)
                            
                            # GPUè¨ˆç®—
                            dataLRB_gpu = cp.outer(dataLavgB_gpu, dataRavgB_gpu)
                            P1_boot_gpu = dataLRB_gpu**n / (k**n + dataLRB_gpu**n)
                            
                            # CPUã«æˆ»ã™
                            P1_boot = cp.asnumpy(P1_boot_gpu)
                            
                            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                            del dataLavgB_gpu, dataRavgB_gpu, dataLRB_gpu, P1_boot_gpu
                            # ãƒ¡ãƒ¢ãƒªè§£æ”¾
                            if idx % 10 == 0:
                                cp.get_default_memory_pool().free_all_blocks()
                        except Exception as e:
                            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                            dataLRB = np.outer(dataLavgB[0, :], dataRavgB[0, :])
                            P1_boot = dataLRB**n / (k**n + dataLRB**n)
                    else:
                        # CPUè¨ˆç®—
                        dataLRB = np.outer(dataLavgB[0, :], dataRavgB[0, :])
                        P1_boot = dataLRB**n / (k**n + dataLRB**n)
                    
                    # ç½®æ›ç”¨ã®ç°¡ç•¥åŒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ«ã¨åŒæ§˜ï¼‰
                    P2_boot = np.ones((numCluster, numCluster))
                    P3_boot = np.ones((numCluster, numCluster))
                    
                    # é›†å›£ã‚µã‚¤ã‚ºåŠ¹æœ
                    P4_boot = np.ones((numCluster, numCluster))
                    if population_size:
                        group_boot = cell_labels.values[permutation[:, j]]
                        cell_proportions_boot = np.array([np.sum(group_boot == ct) for ct in valid_cell_types]) / nC
                        P4_boot = np.outer(cell_proportions_boot, cell_proportions_boot)
                    
                    # æœ€çµ‚ç¢ºç‡
                    Pboot_result = P1_boot * P2_boot * P3_boot * P4_boot
                    batch_results[:, idx] = Pboot_result.flatten()
                
                return batch_results
            
            # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
            for b_start in range(0, nboot, batch_size):
                b_end = min(b_start + batch_size, nboot)
                batch_indices = list(range(b_start, b_end))
                
                if n_jobs_effective > 1 and len(batch_indices) > 1:
                    # ä¸¦åˆ—å‡¦ç† - å„ãƒ¯ãƒ¼ã‚«ãƒ¼ã«GPUä½¿ç”¨ãƒ•ãƒ©ã‚°ã‚’æ¸¡ã™
                    batch_results_list = Parallel(n_jobs=n_jobs_effective, backend="loky")(
                        delayed(compute_permutation_batch)(
                            [j], 
                            ligand_info, 
                            receptor_info,
                            numCluster,
                            n,
                            k,
                            population_size,
                            cell_labels,
                            permutation,
                            valid_cell_types,
                            nC,
                            use_gpu_compute=use_gpu_for_batches and (j % 5 == 0)  # 5ãƒãƒƒãƒã”ã¨ã«GPUä½¿ç”¨
                        ) for j in batch_indices
                    )
                    # çµæœã®çµåˆ
                    for j_idx, j in enumerate(batch_indices):
                        Pboot[:, j] = batch_results_list[j_idx][:, 0]
                else:
                    # å˜ä¸€ã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†
                    batch_results = compute_permutation_batch(
                        batch_indices, 
                        ligand_info, 
                        receptor_info,
                        numCluster,
                        n,
                        k,
                        population_size,
                        cell_labels,
                        permutation,
                        valid_cell_types,
                        nC,
                        use_gpu_compute=use_gpu_for_batches
                    )
                    for j_idx, j in enumerate(batch_indices):
                        Pboot[:, j] = batch_results[:, j_idx]
                
                # GPUãƒ¡ãƒ¢ãƒªè§£æ”¾
                if has_gpu and use_gpu and b_start % 100 == 0:
                    cp.get_default_memory_pool().free_all_blocks()
            
            # på€¤ã®è¨ˆç®— - GPUå¯¾å¿œ
            if has_gpu and use_gpu:
                try:
                    # GPUè¨ˆç®—
                    Pnull_vec_gpu = cp.array(Pnull_vec, dtype=gpu_dtype)
                    Pboot_gpu = cp.array(Pboot, dtype=gpu_dtype)
                    
                    # é–¾å€¤ã‚’è¶…ãˆã‚‹å€¤ã®ã‚«ã‚¦ãƒ³ãƒˆ
                    nReject_gpu = cp.sum(Pboot_gpu > cp.expand_dims(Pnull_vec_gpu, 1), axis=1)
                    p_gpu = nReject_gpu / nboot
                    
                    # CPUè»¢é€
                    p = cp.asnumpy(p_gpu)
                    
                    # æ¤œè¨¼
                    if validate_gpu_results:
                        nReject_cpu = np.sum(Pboot > np.expand_dims(Pnull_vec, 1), axis=1)
                        p_cpu = nReject_cpu / nboot
                        
                        max_diff = np.max(np.abs(p_cpu - p))
                        if max_diff > 1e-5:
                            logger.warning(f"GPU and CPU p-values differ: {max_diff}. Using CPU.")
                            p = p_cpu
                    
                    # ãƒ¡ãƒ¢ãƒªè§£æ”¾
                    del Pnull_vec_gpu, Pboot_gpu, nReject_gpu, p_gpu
                    if i % 50 == 0:
                        cp.get_default_memory_pool().free_all_blocks()
                except Exception as e:
                    logger.warning(f"GPU p-value calculation failed: {str(e)}. Using CPU.")
                    nReject = np.sum(Pboot > np.expand_dims(Pnull_vec, 1), axis=1)
                    p = nReject / nboot
            else:
                # CPUè¨ˆç®—
                nReject = np.sum(Pboot > np.expand_dims(Pnull_vec, 1), axis=1)
                p = nReject / nboot
            
            Pval[:, :, i] = p.reshape(numCluster, numCluster)
        
        # ç¢ºç‡ãŒ0ã®å ´æ‰€ã¯på€¤ã‚’1ã«è¨­å®š
        Pval[Prob == 0] = 1
        
        # på€¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãŒæœ‰åŠ¹ãªã‚‰é©ç”¨
        if apply_pval_filter:
            Prob[Pval > trim_threshold] = 0
        
        # æ¬¡å…ƒåã®è¨­å®š
        dimnames = [list(valid_cell_types), list(valid_cell_types), list(resource.index)]
        
        # ãƒ‘ã‚¹ã‚¦ã‚§ã‚¤ãƒ¬ãƒ™ãƒ«ã§ã®é€šä¿¡ç¢ºç‡ã‚’è¨ˆç®—
        netP = computeCommunProbPathway({"prob": Prob, "pval": Pval}, resource, 
                                       thresh=trim_threshold, apply_pval_filter=apply_pval_filter)
        
        # é›†è¨ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è¨ˆç®—
        net_summary = aggregateCell_Cell_Communication({"prob": Prob, "pval": Pval}, 
                                                     valid_cell_types, pval_threshold=0.05, 
                                                     apply_pval_filter=apply_pval_filter)
        
        logger.info("CellChat minimal analysis completed")
        
        # çµæœãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®æº–å‚™
        results_data = {
            'source': [],
            'target': [],
            'interaction_name': [],
            'ligand': [],
            'receptor': [],
            'prob': [],
            'pval': []
        }
        
        for i in range(len(valid_cell_types)):
            for j in range(len(valid_cell_types)):
                for k in range(nLR):
                    if Prob[i, j, k] > 0:
                        results_data['source'].append(valid_cell_types[i])
                        results_data['target'].append(valid_cell_types[j])
                        results_data['interaction_name'].append(resource.index[k])
                        results_data['ligand'].append(resource['ligand'].iloc[k])
                        results_data['receptor'].append(resource['receptor'].iloc[k])
                        results_data['prob'].append(Prob[i, j, k])
                        results_data['pval'].append(Pval[i, j, k])
        
        results_df = pd.DataFrame(results_data)
        
        # æœ€çµ‚çš„ãªGPUãƒ¡ãƒ¢ãƒªè§£æ”¾
        if has_gpu and use_gpu:
            cp.get_default_memory_pool().free_all_blocks()
        
        # å¯è¦–åŒ–ã«å¿…è¦ãªæœ€å°æ§‹é€ ã‚’è¿”ã™
        return {
            'adata': adata_filtered,
            'results': results_df,
            'net': {
                "prob": Prob,
                "pval": Pval,
                "dimnames": dimnames,
                "centr": None  # ä¸­å¿ƒæ€§ã¯è¨ˆç®—ã—ãªã„ï¼ˆå¯è¦–åŒ–ã«ã¯ä¸è¦ï¼‰
            },
            'netP': netP,
            'network': net_summary,
            'groupby': groupby 
        }
    
    except Exception as e:
        logger.error(f"Error in minimal CellChat analysis: {str(e)}")
        logger.error(traceback.format_exc())
        
        # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚GPUãƒ¡ãƒ¢ãƒªè§£æ”¾
        if has_gpu and use_gpu:
            cp.get_default_memory_pool().free_all_blocks()
            
        return {'error': str(e), 'traceback': traceback.format_exc()}



# Function for splitting AnnData by a grouping variable
def split_anndata(adata, split_key, group1, group2):
    """Split AnnData object into two based on a grouping variable"""
    # Create masks for each group
    mask1 = adata.obs[split_key] == group1
    mask2 = adata.obs[split_key] == group2
    
    # Create subsets
    adata1 = adata[mask1].copy()
    adata2 = adata[mask2].copy()
    
    return adata1, adata2

# Function for finding the first matching index
def find_first_index_or_default(lst, elements, default=0):
    """Find the first matching element in a list or return default index"""
    for element in elements:
        try:
            return lst.index(element)
        except ValueError:
            continue
    return default


def calculate_actual_diff(result1, result2, cell_types):
    """
    Calculate actual differences between two CellChat results
    
    Parameters
    ----------
    result1 : dict
        First CellChat result
    result2 : dict
        Second CellChat result
    cell_types : list
        List of cell types
        
    Returns
    -------
    dict
        Dictionary containing probability and weight differences
    """
    # Prepare dicts for probability and weight differences
    prob_diff = []
    weight_diff = []
    
    # Extract values from results
    if ('netP' in result1 and 'pathways' in result1['netP'] and
        'netP' in result2 and 'pathways' in result2['netP']):
        
        # Get all pathways from both results
        pathways1 = result1['netP']['pathways']
        pathways2 = result2['netP']['pathways']
        all_pathways = sorted(set(pathways1).union(set(pathways2)))
        
        # Get actual cell types from results to handle potential mismatches
        if 'net' in result1 and 'dimnames' in result1['net'] and len(result1['net']['dimnames']) > 0:
            cells1 = result1['net']['dimnames'][0]
        else:
            cells1 = []
            
        if 'net' in result2 and 'dimnames' in result2['net'] and len(result2['net']['dimnames']) > 0:
            cells2 = result2['net']['dimnames'][0]
        else:
            cells2 = []
        
        # Use intersection of cell types to avoid index errors
        common_cells = [ct for ct in cell_types if ct in cells1 and ct in cells2]
        
        # Print debug info
        print(f"Cell types from input: {cell_types}")
        print(f"Cell types from result1: {cells1}")
        print(f"Cell types from result2: {cells2}")
        print(f"Common cell types: {common_cells}")
        
        if not common_cells:
            st.warning("No common cell types found between the two results!")
            # Use all provided cell types as fallback
            common_cells = cell_types
        
        # For each pathway, get probability differences
        for pathway in all_pathways:
            pathway_idx1 = list(pathways1).index(pathway) if pathway in pathways1 else None
            pathway_idx2 = list(pathways2).index(pathway) if pathway in pathways2 else None
            
            # Get the indices within each result
            for source in common_cells:
                for target in common_cells:
                    # Get source and target indices in each result
                    if source in cells1 and target in cells1:
                        i1 = cells1.index(source)
                        j1 = cells1.index(target)
                    else:
                        i1 = j1 = None
                        
                    if source in cells2 and target in cells2:
                        i2 = cells2.index(source)
                        j2 = cells2.index(target)
                    else:
                        i2 = j2 = None
                    
                    # Get probability values
                    prob1 = (result1['netP']['prob'][i1, j1, pathway_idx1] 
                             if i1 is not None and j1 is not None and pathway_idx1 is not None else 0)
                    prob2 = (result2['netP']['prob'][i2, j2, pathway_idx2] 
                             if i2 is not None and j2 is not None and pathway_idx2 is not None else 0)
                    
                    prob_diff.append({
                        'source': source,
                        'target': target,
                        'pathway': pathway,
                        'actual_prob_diff': prob2 - prob1  # Group 2 - Group 1
                    })
    
    # Calculate weight differences
    if ('network' in result1 and 'strength_matrix' in result1['network'] and 
        'network' in result2 and 'strength_matrix' in result2['network']):
        
        matrix1 = result1['network']['strength_matrix']
        matrix2 = result2['network']['strength_matrix']
        
        # Use intersection of cell types
        common_cells = [ct for ct in cell_types if ct in matrix1.index and ct in matrix1.columns
                      and ct in matrix2.index and ct in matrix2.columns]
                      
        if not common_cells:
            st.warning("No common cell types found in strength matrices!")
            # Fallback to provided cell types with checks
            for source in cell_types:
                for target in cell_types:
                    if (source in matrix1.index and target in matrix1.columns and
                        source in matrix2.index and target in matrix2.columns):
                        
                        weight1 = matrix1.loc[source, target]
                        weight2 = matrix2.loc[source, target]
                        
                        weight_diff.append({
                            'source': source,
                            'target': target,
                            'actual_weight_diff': weight2 - weight1  # Group 2 - Group 1
                        })
        else:
            # Use common cells
            for source in common_cells:
                for target in common_cells:
                    weight1 = matrix1.loc[source, target]
                    weight2 = matrix2.loc[source, target]
                    
                    weight_diff.append({
                        'source': source,
                        'target': target,
                        'actual_weight_diff': weight2 - weight1  # Group 2 - Group 1
                    })
    
    return {
        'prob_diff': pd.DataFrame(prob_diff),
        'weight_diff': pd.DataFrame(weight_diff)
    }

def calculate_p_values(actual_diff, perm_results):
    """
    Calculate p-values based on permutation results
    
    Parameters
    ----------
    actual_diff : dict
        Dictionary containing actual differences
    perm_results : list
        List of permutation result dictionaries
        
    Returns
    -------
    dict
        Dictionary containing p-values for probability and weight differences
    """
    # Initialize DataFrames to store p-values
    prob_p_values = actual_diff['prob_diff'].copy()
    weight_p_values = actual_diff['weight_diff'].copy()
    
    # Add columns for p-values
    prob_p_values['p_value_prob'] = 1.0
    weight_p_values['p_value_weight'] = 1.0
    
    # Initialize counting dictionaries for calculating p-values
    prob_counts = {}
    for _, row in prob_p_values.iterrows():
        key = (row['source'], row['target'], row['pathway'])
        prob_counts[key] = {
            'greater_equal': 0,
            'less_equal': 0
        }
    
    weight_counts = {}
    for _, row in weight_p_values.iterrows():
        key = (row['source'], row['target'])
        weight_counts[key] = {
            'greater_equal': 0,
            'less_equal': 0
        }
    
    # Count permutation results
    for perm_result in perm_results:
        # Count for probability
        for _, row in perm_result['prob_diff'].iterrows():
            key = (row['source'], row['target'], row['pathway'])
            if key in prob_counts:
                perm_diff = row['actual_prob_diff']
                prob_counts[key]['greater_equal'] += (perm_diff >= 0)
                prob_counts[key]['less_equal'] += (perm_diff <= 0)
        
        # Count for weight
        for _, row in perm_result['weight_diff'].iterrows():
            key = (row['source'], row['target'])
            if key in weight_counts:
                perm_diff = row['actual_weight_diff']
                weight_counts[key]['greater_equal'] += (perm_diff >= 0)
                weight_counts[key]['less_equal'] += (perm_diff <= 0)
    
    # Calculate p-values
    n_perms = len(perm_results)
    
    # For probability
    for i, row in prob_p_values.iterrows():
        key = (row['source'], row['target'], row['pathway'])
        actual_diff_val = row['actual_prob_diff']
        
        if actual_diff_val >= 0:
            p_val = prob_counts[key]['greater_equal'] / n_perms
        else:
            p_val = prob_counts[key]['less_equal'] / n_perms
        
        prob_p_values.at[i, 'p_value_prob'] = p_val
    
    # For weight
    for i, row in weight_p_values.iterrows():
        key = (row['source'], row['target'])
        actual_diff_val = row['actual_weight_diff']
        
        if actual_diff_val >= 0:
            p_val = weight_counts[key]['greater_equal'] / n_perms
        else:
            p_val = weight_counts[key]['less_equal'] / n_perms
        
        weight_p_values.at[i, 'p_value_weight'] = p_val
    
    return {
        'p_values_prob': prob_p_values,
        'p_values_weight': weight_p_values
    }

def adjust_p_values(p_values):
    """
    Adjust p-values for multiple testing
    
    Parameters
    ----------
    p_values : dict
        Dictionary containing p-values
        
    Returns
    -------
    dict
        Dictionary containing adjusted p-values
    """
    # Adjust probability p-values
    prob_adjusted = p_values['p_values_prob'].copy()
    prob_adjusted['p_adj_bonferroni_prob'] = multipletests(
        prob_adjusted['p_value_prob'], method='bonferroni'
    )[1]
    prob_adjusted['p_adj_BH_prob'] = multipletests(
        prob_adjusted['p_value_prob'], method='fdr_bh'
    )[1]
    
    # Adjust weight p-values
    weight_adjusted = p_values['p_values_weight'].copy()
    weight_adjusted['p_adj_bonferroni_weight'] = multipletests(
        weight_adjusted['p_value_weight'], method='bonferroni'
    )[1]
    weight_adjusted['p_adj_BH_weight'] = multipletests(
        weight_adjusted['p_value_weight'], method='fdr_bh'
    )[1]
    
    return {
        'p_values_prob_adjusted': prob_adjusted,
        'p_values_weight_adjusted': weight_adjusted
    }

def plot_p_value_distribution(p_values_adjusted):
    """
    Plot the distribution of p-values
    
    Parameters
    ----------
    p_values_adjusted : dict
        Dictionary containing adjusted p-values
        
    Returns
    -------
    tuple
        Tuple of matplotlib figures
    """
    # Probability p-values
    fig_prob, ax_prob = plt.subplots(figsize=(10, 6))
    sns.histplot(p_values_adjusted['p_values_prob_adjusted']['p_value_prob'], 
                bins=50, color='blue', alpha=0.7, ax=ax_prob)
    ax_prob.set_title('Distribution of P-values (Probability)')
    ax_prob.set_xlabel('P-value')
    ax_prob.set_ylabel('Count')
    
    # Weight p-values
    fig_weight, ax_weight = plt.subplots(figsize=(10, 6))
    sns.histplot(p_values_adjusted['p_values_weight_adjusted']['p_value_weight'], 
                bins=50, color='red', alpha=0.7, ax=ax_weight)
    ax_weight.set_title('Distribution of P-values (Weight)')
    ax_weight.set_xlabel('P-value')
    ax_weight.set_ylabel('Count')
    
    return fig_prob, fig_weight

def filter_significant_results(p_values_adjusted, threshold=0.05):
    """
    Filter significant results based on adjusted p-values
    
    Parameters
    ----------
    p_values_adjusted : dict
        Dictionary containing adjusted p-values
    threshold : float
        P-value threshold
        
    Returns
    -------
    dict
        Dictionary containing significant results
    """
    # Filter probability results
    significant_prob = p_values_adjusted['p_values_prob_adjusted'][
        p_values_adjusted['p_values_prob_adjusted']['p_adj_BH_prob'] < threshold
    ].sort_values('p_adj_BH_prob')
    
    # Filter weight results
    significant_weight = p_values_adjusted['p_values_weight_adjusted'][
        p_values_adjusted['p_values_weight_adjusted']['p_adj_BH_weight'] < threshold
    ].sort_values('p_adj_BH_weight')
    
    return {
        'prob': significant_prob,
        'weight': significant_weight
    }

def create_refined_heatmap(data, diff_var, p_value_var, title, n_perm=1000, is_pathway=False, cell_types=None, cell_order=None):
    """
    Create a refined heatmap showing differences and p-values
    
    Parameters
    ----------
    data : pd.DataFrame
        DataFrame containing difference and p-value data
    diff_var : str
        Column name for difference values
    p_value_var : str
        Column name for p-values
    title : str
        Plot title
    n_perm : int
        Number of permutations performed (default=1000)
    is_pathway : bool
        Whether the data is pathway-specific
    cell_types : list
        List of cell types to include
    cell_order : list
        List specifying the order of cell types
        
    Returns
    -------
    matplotlib.figure.Figure
        The heatmap figure
    """
    # Create a copy of the data
    plot_data = data.copy()
    
    # Debug: Print some data information
    print(f"Data shape: {plot_data.shape}")
    print(f"P-value column: {p_value_var}")
    print(f"Number of permutations: {n_perm}")
    
    # Determine the minimum achievable p-value and max log scale based on permutation count
    if n_perm <= 10:
        min_p_value = 0.1
        max_log_p = 1  # -log10(0.1) = 1
        p_thresholds = [0.1]  # Only show 0.1 in legend
    elif n_perm <= 100:
        min_p_value = 0.01
        max_log_p = 2  # -log10(0.01) = 2
        p_thresholds = [0.1, 0.01]  # Show 0.1 and 0.01 in legend
    else:
        min_p_value = 0.001
        max_log_p = 3  # -log10(0.001) = 3
        p_thresholds = [0.1, 0.01, 0.001]  # Show all thresholds in legend
    
    print(f"Minimum p-value for {n_perm} permutations: {min_p_value}")
    print(f"Max -log10(p) value: {max_log_p}")
    print(f"P-value thresholds to display in legend: {p_thresholds}")
    
    # Use cell_order if provided, otherwise use cell_types
    if cell_order is not None:
        ordered_cells = cell_order
    else:
        ordered_cells = cell_types
    
    # Ensure we have all source and target combinations
    if cell_types is not None:
        # Create a grid of all source and target combinations
        all_combinations = []
        for source in ordered_cells:
            for target in ordered_cells:
                if source in cell_types and target in cell_types:
                    all_combinations.append({
                        'source': source,
                        'target': target
                    })
        
        all_combinations_df = pd.DataFrame(all_combinations)
        
        # Merge with the actual data
        plot_data = pd.merge(
            all_combinations_df, plot_data, on=['source', 'target'], how='left'
        )
        
        # Fill NA values
        plot_data[diff_var] = plot_data[diff_var].fillna(0)
        plot_data[p_value_var] = plot_data[p_value_var].fillna(1)
    
    # Create a pivot table for the heatmap
    pivot_data = plot_data.pivot_table(
        index='source', columns='target', values=diff_var, fill_value=0
    )
    
    # Create a pivot table for p-values
    pivot_pvals = plot_data.pivot_table(
        index='source', columns='target', values=p_value_var, fill_value=1
    )
    
    # Reorder rows and columns if custom order is provided
    if cell_order is not None:
        # Filter to include only cells that exist in the pivot table
        valid_ordered_cells = [cell for cell in ordered_cells if cell in pivot_data.index]
        
        # Reorder the rows and columns
        pivot_data = pivot_data.reindex(index=valid_ordered_cells, columns=valid_ordered_cells)
        pivot_pvals = pivot_pvals.reindex(index=valid_ordered_cells, columns=valid_ordered_cells)
    
    # Set up the figure
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # Calculate max absolute value for symmetric color scale
    vmax = np.max(np.abs(pivot_data.values))
    vmin = -vmax
    
    # Define custom colormap similar to the R implementation
    # Using the custom colormap that matches image: teal for negative, white for zero, red for positive
    cmap = sns.diverging_palette(200, 10, s=99, l=55, sep=10, as_cmap=True)  # Teal to red
    
    # Create the heatmap with light grey background for the grid
    ax.set_facecolor('#f5f5f5')  # Light grey background
    
    # Create the heatmap with a smaller colorbar and wider width
    cbar_kws = {
        'label': 'Difference',
        'shrink': 0.4,     # Make colorbar 40% of original height
        'aspect': 15,      # Lower aspect ratio makes colorbar wider
        'pad': 0.02        # Less space between heatmap and colorbar
    }
    
    heatmap = sns.heatmap(
        pivot_data, 
        cmap=cmap, 
        center=0,
        vmin=vmin, 
        vmax=vmax,
        annot=False, 
        fmt='.2f',
        linewidths=0.5,
        linecolor='white',
        ax=ax,
        cbar_kws=cbar_kws
    )
    
    # Base size for dots
    base_size = 20  # Large base size for visibility
    
    # Count dots for debugging
    dot_count = 0
    p_value_counts = {0.1: 0, 0.01: 0, 0.001: 0}
    
    # Add dots with continuous scaling
    for i in range(pivot_data.shape[0]):
        for j in range(pivot_data.shape[1]):
            p_val = pivot_pvals.iloc[i, j]
            
            # Skip if invalid p-value
            if p_val >= 1 or pd.isna(p_val):
                continue
            
            # Calculate -log10(p) value capped at max_log_p
            log_p_val = min(-np.log10(p_val), max_log_p)
            
            # Count which p-value threshold this dot falls under
            for threshold in sorted([0.1, 0.01, 0.001], reverse=True):
                if p_val <= threshold:
                    p_value_counts[threshold] += 1
                    break
            
            # Continuous scaling from 0 to max_log_p, mapped to size 0.25 to 1.0
            # This ensures the smallest p-value gets the largest dot
            if log_p_val >= 0:
                # Normalize to range [0, 1] based on max_log_p
                normalized = log_p_val / max_log_p
                
                # Scale to range [0.25, 1.0] for dot sizes
                scaled_size = 0.25 + 0.75 * normalized
                
                # Calculate final dot size
                final_size = base_size * scaled_size
                
                # Add a black dot
                ax.plot(j + 0.5, i + 0.5, 'o', markersize=final_size,
                        color='black', alpha=1.0)
                dot_count += 1
    
    # Debug: Print dot information
    print(f"Total dots plotted: {dot_count}")
    for p, count in p_value_counts.items():
        if count > 0:
            print(f"  p <= {p}: {count} dots")
    
    # Rotate x-axis labels for better readability
    plt.xticks(rotation=90)
    
    # Add axis labels
    ax.set_xlabel('target')
    ax.set_ylabel('source')
    
    # Set title
    ax.set_title(title, fontsize=14)
    
    # Create a legend for p-value thresholds
    legend_elements = []
    
    # Create legend elements for the available thresholds
    for p in p_thresholds:
        # Calculate -log10(p)
        log_p = -np.log10(p)
        
        # Normalize to range [0, 1] based on max_log_p
        normalized = log_p / max_log_p
        
        # Scale to range [0.25, 1.0] for dot sizes, same as in the plot
        scaled_size = 0.25 + 0.75 * normalized
        
        # Calculate legend size
        legend_size = base_size * scaled_size
        
        legend_elements.append(
            Line2D([0], [0], marker='o', color='w', markerfacecolor='black',
                   markersize=legend_size, label=f'{p}')
        )
    
    # Add the legend at bottom right of the heatmap
    p_legend = ax.legend(
        handles=legend_elements, 
        title="p-value", 
        loc='lower right',
        bbox_to_anchor=(0.98, 0.02),  # Position at bottom right
        fontsize=8,         
        title_fontsize=9,   
        frameon=True,
        framealpha=0.8,     
        ncol=1,             # Vertical layout
        borderpad=0.3,      
        handletextpad=0.5,  
        columnspacing=0.8,  
        labelspacing=0.3    
    )
    
    # Make room for the colorbar and legends
    fig.tight_layout(rect=[0, 0, 0.88, 0.95])
    
    return fig

def suggest_optimal_n_jobs():
    """
    Suggest optimal number of jobs based on available CPU cores
    
    Returns
    -------
    int
        Suggested optimal number of jobs
    """
    total_cores = os.cpu_count()
    
    # For less than 8 cores, use all but 1
    if total_cores <= 8:
        return max(1, total_cores - 1)
    
    # For 8-32 cores, use half of available cores
    if total_cores <= 32:
        return max(8, total_cores // 2)
    
    # For more than 32 cores, use 16 cores
    return 16

def display_visualizations(perm_results, cell_types, group1, group2, significance_threshold=0.05):
    """
    Display visualizations based on permutation results
    
    Parameters
    ----------
    perm_results : dict
        Permutation analysis results
    cell_types : list
        List of cell types
    group1 : str
        Name of first group
    group2 : str
        Name of second group
    significance_threshold : float
        P-value threshold for significance
    
    Returns
    -------
    None
    """
    # Get number of permutations from the results (if available, otherwise use default)
    n_perms = 1000  # Default value
    if 'permutation_results' in perm_results:
        n_perms = len(perm_results['permutation_results'])
        st.write(f"Analysis based on {n_perms} permutations")
    
    # Create tabs for visualization
    results_tabs = st.tabs([
        "P-value Distribution", 
        "Significant Results",
        "Weight Heatmap",
        "Pathway Heatmaps"
    ])
    
    # Tab 1: P-value Distribution
    with results_tabs[0]:
        st.header("P-value Distribution")
        
        col1, col2 = st.columns(2)
        
        # Plot the p-value distribution
        fig_prob, fig_weight = plot_p_value_distribution(perm_results['adjusted_p_values'])
        
        with col1:
            st.pyplot(fig_prob)
            
            # Save and download
            prob_pdf_path = f"{cellchat_temp_dir}/pval_distribution_prob.pdf"
            fig_prob.savefig(prob_pdf_path, bbox_inches='tight')
            
            with open(prob_pdf_path, "rb") as pdf_file:
                st.download_button(
                    label="Download Probability P-value Distribution",
                    data=pdf_file.read(),
                    file_name="pval_distribution_prob.pdf",
                    mime="application/pdf"
                )
        
        with col2:
            st.pyplot(fig_weight)
            
            # Save and download
            weight_pdf_path = f"{cellchat_temp_dir}/pval_distribution_weight.pdf"
            fig_weight.savefig(weight_pdf_path, bbox_inches='tight')
            
            with open(weight_pdf_path, "rb") as pdf_file:
                st.download_button(
                    label="Download Weight P-value Distribution",
                    data=pdf_file.read(),
                    file_name="pval_distribution_weight.pdf",
                    mime="application/pdf"
                )
    
    # Tab 2: Significant Results
    with results_tabs[1]:
        st.header("Significant Results")
        
        # Significance threshold
        local_threshold = st.slider(
            "Significance threshold (adjusted p-value):",
            min_value=0.001,
            max_value=0.1,
            value=significance_threshold,
            step=0.001,
            key="significance_threshold_slider"
        )
        
        # Filter significant results
        significant_results = filter_significant_results(
            perm_results['adjusted_p_values'],
            threshold=local_threshold
        )
        
        # Display results in tabs
        sig_tabs = st.tabs(["Probability", "Weight"])
        
        with sig_tabs[0]:
            st.subheader(f"Significant Probability Differences (BH-adjusted p < {local_threshold})")
            
            if len(significant_results['prob']) > 0:
                st.write(f"Found {len(significant_results['prob'])} significant probability differences")
                
                # Display the top significant results
                st.dataframe(
                    significant_results['prob'][['source', 'target', 'pathway', 
                                              'actual_prob_diff', 'p_value_prob', 
                                              'p_adj_BH_prob']]
                )
                
                # Download as CSV
                csv = significant_results['prob'].to_csv(index=False)
                st.download_button(
                    label="Download significant probability results as CSV",
                    data=csv,
                    file_name="significant_probability_differences.csv",
                    mime="text/csv"
                )
            else:
                st.write("No significant probability differences found at this threshold.")
        
        with sig_tabs[1]:
            st.subheader(f"Significant Weight Differences (BH-adjusted p < {local_threshold})")
            
            if len(significant_results['weight']) > 0:
                st.write(f"Found {len(significant_results['weight'])} significant weight differences")
                
                # Display the top significant results
                st.dataframe(
                    significant_results['weight'][['source', 'target', 
                                               'actual_weight_diff', 'p_value_weight', 
                                               'p_adj_BH_weight']]
                )
                
                # Download as CSV
                csv = significant_results['weight'].to_csv(index=False)
                st.download_button(
                    label="Download significant weight results as CSV",
                    data=csv,
                    file_name="significant_weight_differences.csv",
                    mime="text/csv"
                )
            else:
                st.write("No significant weight differences found at this threshold.")
    
    # Tab 3: Weight Heatmap
    with results_tabs[2]:
        st.header("Weight Difference Heatmap")
        
        # Create the heatmap with custom cell order if available
        fig_weight_heatmap = create_refined_heatmap(
            perm_results['adjusted_p_values']['p_values_weight_adjusted'],
            'actual_weight_diff',
            'p_value_weight',
            f"Weight Differences: {group2} vs {group1}",
            cell_types=cell_types,
            cell_order=st.session_state.cell_type_order,
            n_perm=n_perms
        )
        
        st.pyplot(fig_weight_heatmap)
        
        # Save and download
        weight_heatmap_path = f"{cellchat_temp_dir}/weight_difference_heatmap.pdf"
        fig_weight_heatmap.savefig(weight_heatmap_path, bbox_inches='tight')
        
        with open(weight_heatmap_path, "rb") as pdf_file:
            st.download_button(
                label="Download Weight Difference Heatmap",
                data=pdf_file.read(),
                file_name="weight_difference_heatmap.pdf",
                mime="application/pdf"
            )
    
    # Tab 4: Pathway Heatmaps
    with results_tabs[3]:
        st.header("Pathway-Specific Heatmaps")
        
        # Get all available pathways
        if 'result1' in perm_results and 'netP' in perm_results['result1'] and 'pathways' in perm_results['result1']['netP']:
            pathways1 = set(perm_results['result1']['netP']['pathways'])
        else:
            pathways1 = set()
            
        if 'result2' in perm_results and 'netP' in perm_results['result2'] and 'pathways' in perm_results['result2']['netP']:
            pathways2 = set(perm_results['result2']['netP']['pathways'])
        else:
            pathways2 = set()
            
        all_pathways = sorted(pathways1.union(pathways2))
        
        if not all_pathways:
            st.warning("No pathways available for heatmap visualization.")
        else:
            # Initialize session state for pathway selection if not exists
            if 'selected_pathway' not in st.session_state:
                st.session_state.selected_pathway = all_pathways[0]
                
            # Pathway selection with callback function to update session state
            def on_pathway_change():
                # This function is called when the selectbox value changes
                # The new value is already in st.session_state.pathway_selector
                st.session_state.selected_pathway = st.session_state.pathway_selector
                
            selected_pathway = st.selectbox(
                "Select pathway for heatmap:",
                all_pathways,
                key="pathway_selector",
                index=all_pathways.index(st.session_state.selected_pathway) if st.session_state.selected_pathway in all_pathways else 0,
                on_change=on_pathway_change
            )
            
            # Use the pathway from session state
            current_pathway = st.session_state.selected_pathway
            
            # Filter data for the selected pathway
            pathway_data = perm_results['adjusted_p_values']['p_values_prob_adjusted'][
                perm_results['adjusted_p_values']['p_values_prob_adjusted']['pathway'] == current_pathway
            ]
            
            # Create the heatmap with custom cell order if available
            fig_pathway = create_refined_heatmap(
                pathway_data,
                'actual_prob_diff',
                'p_value_prob',
                f"Probability Differences - {current_pathway}: {group2} vs {group1}",
                is_pathway=True,
                cell_types=cell_types,
                cell_order=st.session_state.cell_type_order,
                n_perm=n_perms
            )
            
            st.pyplot(fig_pathway)
            
            # Save and download
            pathway_pdf_path = f"{cellchat_temp_dir}/pathway_{current_pathway}_heatmap.pdf"
            fig_pathway.savefig(pathway_pdf_path, bbox_inches='tight')
            
            with open(pathway_pdf_path, "rb") as pdf_file:
                st.download_button(
                    label=f"Download {current_pathway} Pathway Heatmap",
                    data=pdf_file.read(),
                    file_name=f"pathway_{current_pathway}_heatmap.pdf",
                    mime="application/pdf"
                )

if __name__ == "__main__":
    """Main function to run CellChat permutation comparison"""
    # Main title
    st.title("CellChat Permutation Comparison Analysis")

    # Initialize session state variables if they don't exist
    if 'analysis_parameters' not in st.session_state:
        st.session_state.analysis_parameters = {}
    if 'analysis_completed' not in st.session_state:
        st.session_state.analysis_completed = False
    if 'current_mode' not in st.session_state:
        st.session_state.current_mode = "setup"  # "setup", "running", or "visualization"
    if 'cell_type_order' not in st.session_state:
        st.session_state.cell_type_order = None

    # ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ãƒ¡ã‚¤ãƒ³éƒ¨åˆ†ï¼ˆä¸¦åˆ—å‡¦ç†ã‚’é–‹å§‹ã™ã‚‹å‰ï¼‰ã«è¿½åŠ 
    import rpy2.robjects as ro
    from rpy2.rinterface_lib import callbacks

    # Rã‚’ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã§åˆæœŸåŒ–
    ro.r('library(methods)')

    # Create sidebar
    with st.sidebar:
        st.title("Settings")
        
        # Cell type order settings (only shown when cell types are available)
        if 'cell_types' in st.session_state:
            st.header("Cell Type Display Settings")
            sort_cell = st.checkbox("Change cell type order?", key="sort_cell_checkbox")
            
            if sort_cell:
                with st.form("Sorter"):
                    sorted_cell_types = sort_items(sorted(st.session_state.cell_types))
                    submitted_sort = st.form_submit_button("Done sorting")
                st.session_state.cell_type_order = sorted_cell_types
                    
            else:
                # Reset to default order
                st.session_state.cell_type_order = None
    
    # Main panel
    st.header("Data Settings")

    uploaded_file = st.file_uploader("Upload H5AD file", type=['h5ad'], 
                                     help="AnnData file for CellChat analysis")

    if uploaded_file is not None:
        # Try to read the file
        try:
            if 'adata' not in st.session_state or st.session_state.get('uploaded_file_name') != uploaded_file.name:
                with st.spinner('Reading AnnData file...'):
                    adata = sc.read_h5ad(uploaded_file)
                st.session_state.adata = adata
                st.session_state.uploaded_file_name = uploaded_file.name
                # Reset analysis state when a new file is uploaded
                st.session_state.analysis_completed = False
                st.session_state.current_mode = "setup"
                st.session_state.analysis_parameters = {}
                if 'perm_results' in st.session_state:
                    del st.session_state.perm_results
            else:
                adata = st.session_state.adata
            
            st.success(f"Successfully loaded file: {uploaded_file.name}")
            
            # Display summary info
            st.write(f"Cells: {adata.n_obs}, Genes: {adata.n_vars}")
            
            # Show available categorical variables for cell type
            categorical_cols = [col for col in adata.obs.columns if isinstance(adata.obs[col].dtype, pd.CategoricalDtype)]
            other_cols = [col for col in adata.obs.columns if col not in categorical_cols]
            
            # Select cell type annotation column
            default_celltype_idx = find_first_index_or_default(
                categorical_cols, 
                ['cell.ident', 'cell_type', 'celltype', 'seurat_clusters'],
                0
            )
            celltype_key = st.selectbox(
                "Select cell type annotation column:",
                categorical_cols,
                index=min(default_celltype_idx, len(categorical_cols)-1) if categorical_cols else 0,
                help="Column to use for cell type annotations"
            )
            cell_list = sorted(adata.obs[celltype_key].cat.categories.to_list() if hasattr(adata.obs[celltype_key], 'cat') else sorted(adata.obs[celltype_key].unique()))
            st.write(", ".join(cell_list))

            # Select group comparison column
            default_group_idx = find_first_index_or_default(
                categorical_cols, 
                ['orig.ident', 'stimulus', 'condition', 'treatment', 'sample'],
                0
            )
            splitby_key = st.selectbox(
                "Select grouping variable for comparison:",
                categorical_cols,
                index=min(default_group_idx, len(categorical_cols)-1) if categorical_cols else 0,
                help="Column to split the dataset for comparison"
            )

            # Get available groups from the selected grouping variable
            if splitby_key:
                available_groups = adata.obs[splitby_key].cat.categories.tolist() if hasattr(adata.obs[splitby_key], 'cat') else sorted(adata.obs[splitby_key].unique())
                
                # If there are exactly 2 groups, select them by default
                if len(available_groups) == 2:
                    group1 = available_groups[0]
                    group2 = available_groups[1]
                else:
                    # Otherwise let the user select
                    group1 = st.selectbox("Select first group", available_groups, index=0)
                    remaining_groups = [g for g in available_groups if g != group1]
                    group2 = st.selectbox("Select second group", remaining_groups, index=0)
                
                # Display cell counts for selected groups
                g1_count = (adata.obs[splitby_key] == group1).sum()
                g2_count = (adata.obs[splitby_key] == group2).sum()
                st.write(f"Group 1 ({group1}): {g1_count} cells")
                st.write(f"Group 2 ({group2}): {g2_count} cells")
            
            # Available data layers
            available_layers = list(adata.layers.keys()) if hasattr(adata, 'layers') else []
            st.write(f"Available data layers: {', '.join(available_layers) if available_layers else 'No additional layers'}")
            
            data_layer = st.selectbox(
                "Select data layer:",
                ['X (default)'] + available_layers,
                index=0,
                help="Data layer to use for analysis"
            )
            
            # Infer species from gene names
            species = st.radio("Species:", ('mouse', 'human'), 
                              index=check_species_index(adata.var.index.to_list()[:50]))
            
            # Analysis settings 
            st.header("Analysis Settings")
            
            with st.expander("CellChat Analysis Settings", expanded=True):
                col1, col2 = st.columns(2)
                
                with col1:
                    selected_types = st.multiselect(
                        "Signaling types to analyze:",
                        ["Secreted Signaling", "ECM-Receptor", "Cell-Cell Contact", "Non-protein Signaling"],
                        default=["Secreted Signaling", "ECM-Receptor", "Cell-Cell Contact"],
                        help="Types of cellular communication to include in the analysis"
                    )
                    
                    expr_prop = st.number_input(
                        'Min fraction of expressing cells:',
                        min_value=0.0, max_value=0.9, step=0.01, value=0.1,
                        help="Minimum fraction of cells expressing a gene to be considered"
                    )
                    
                    thresh_p = st.number_input(
                        'P-value threshold for differential expression:',
                        min_value=0.0, max_value=0.9, step=0.01, value=0.05,
                        help="Threshold for Wilcoxon test for overexpressed genes"
                    )
                    
                    min_cells = st.number_input(
                        'Minimum cells per group:',
                        min_value=5, max_value=50, step=5, value=10,
                        help="Minimum number of cells required in each group"
                    )
                
                with col2:
                    # Permutation settings
                    n_perms = st.radio(
                        "Number of permutations:",
                        [10, 50, 100, 500, 1000],
                        index=1,  # Default to 50
                        help="Number of permutations for statistical testing"
                    )
                    
                    optimal_n_jobs = suggest_optimal_n_jobs()
                    st.info(f"For best performance, we recommend using {optimal_n_jobs} CPU cores.")
                    
                    n_cpus = st.slider(
                        "Number of CPUs:",
                        min_value=1, max_value=os.cpu_count(), step=1, value=optimal_n_jobs,
                        help="Number of CPU cores to use for parallel processing"
                    )
                
                # Store current settings
                current_settings = {
                    'celltype_key': celltype_key,
                    'splitby_key': splitby_key,
                    'group1': group1,
                    'group2': group2,
                    'data_layer': None if data_layer == 'X (default)' else data_layer,
                    'species': species,
                    'selected_types': selected_types,
                    'expr_prop': expr_prop,
                    'thresh_p': thresh_p,
                    'min_cells': min_cells,
                    'n_perms': n_perms,
                    'n_cpus': n_cpus,
                    'cell_list': cell_list
                }
                
                # Add "Run Analysis" button only if needed
                if not st.session_state.analysis_completed or current_settings != st.session_state.analysis_parameters:
                    # Save current settings for comparison
                    st.session_state.analysis_parameters = current_settings.copy()
                    
                    if st.button("Run Analysis", key="run_analysis"):
                        st.session_state.current_mode = "running"
                        st.rerun()
            
            # Conditional display based on current mode
            if st.session_state.current_mode == "running":
                # Run the analysis with current parameters
                try:
                    parameters = st.session_state.analysis_parameters
                    
                    # Extract parameters
                    celltype_key = parameters['celltype_key']
                    splitby_key = parameters['splitby_key']
                    group1 = parameters['group1']
                    group2 = parameters['group2']
                    data_layer = parameters['data_layer']
                    species = parameters['species']
                    selected_types = parameters['selected_types']
                    expr_prop = parameters['expr_prop']
                    thresh_p = parameters['thresh_p']
                    min_cells = parameters['min_cells']
                    n_perms = parameters['n_perms']
                    n_cpus = parameters['n_cpus']
                    cell_list = parameters['cell_list']
                    
                    # Split AnnData into two groups
                    with st.spinner(f"Splitting data into {group1} and {group2} groups..."):
                        adata1, adata2 = split_anndata(adata, splitby_key, group1, group2)
                        
                        # Check if either group has too few cells
                        if adata1.n_obs < min_cells:
                            st.error(f"Group '{group1}' has fewer than {min_cells} cells.")
                            st.stop()
                        if adata2.n_obs < min_cells:
                            st.error(f"Group '{group2}' has fewer than {min_cells} cells.")
                            st.stop()
                        
                        st.success(f"Successfully split data: {group1} ({adata1.n_obs} cells) and {group2} ({adata2.n_obs} cells)")

                                        # (1) adata1 ã§ DEè§£æ
                    with st.spinner("Performing DE analysis for Group1..."):
                        de_result_g1 = identify_overexpressed_genes(
                            adata1,
                            group_by=celltype_key,  # ä¾‹: adata1ã®ä¸­ã®celltype(ã‚ã‚‹ã„ã¯åŒã˜splitby_key)
                            do_de=True,
                            do_fast=True,
                            thresh_p=thresh_p,
                            thresh_pct=expr_prop,
                            min_cells=min_cells
                        )
                        feature_list_g1 = set(de_result_g1["features"])
                        st.write(f"Group1 DE: found {len(feature_list_g1)} genes")

                    # (2) adata2 ã§ DEè§£æ
                    with st.spinner("Performing DE analysis for Group2..."):
                        de_result_g2 = identify_overexpressed_genes(
                            adata2,
                            group_by=celltype_key,  # ä¾‹: adata2ã®ä¸­ã§ã‚‚åŒã˜ã‚­ãƒ¼ã‚’åˆ©ç”¨
                            do_de=True,
                            do_fast=True,
                            thresh_p=thresh_p,
                            thresh_pct=expr_prop,
                            min_cells=min_cells
                        )
                        feature_list_g2 = set(de_result_g2["features"])
                        st.write(f"Group2 DE: found {len(feature_list_g2)} genes")

                    # (3) unionã‚’å–ã‚‹
                    feature_union = feature_list_g1.union(feature_list_g2)
                    feature_list = list(feature_union)
                    st.write(f"Union of Group1/Group2 DE genes => {len(feature_list)} genes total")

                    
                    # Get CellChatDB
                    with st.spinner(f"Getting {species} CellChatDB..."):
                        cellchatdb = get_cellchatdb_from_r(species=species)
                        cellchatdb = debug_cellchatdb(cellchatdb)
                        st.success(f"{species} CellChatDB successfully obtained")
                        
                        # Filter CellChatDB to selected signaling types
                        interaction = cellchatdb['interaction']
                        interaction_filtered = interaction[interaction['annotation'].isin(selected_types)]
                        cellchatdb['interaction'] = interaction_filtered
                        
                        st.write(f"Using {len(cellchatdb['interaction'])} ligand-receptor pairs")
                    
                    # Run permutation analysis
                    with st.spinner(f"Running permutation analysis with {n_perms} permutations..."):
                        # Set up CellChat parameters
                        cellchat_params = {
                            'use_layer': data_layer,
                            'min_cells': min_cells,
                            'expr_prop': expr_prop,
                            'pseudocount': 1.0,
                            'trim_threshold': 0.05,
                            'k': 0.5,
                            'n': 1,
                            'type_mean': "triMean",
                            'raw_use': True,
                            'population_size': False,
                            'nboot': 100,  # Internal boostrapping in CellChat
                            'seed': 12345,
                            'key_added': "cellchat_res",
                            'trim': 0.1,
                            'apply_pval_filter': False,
                            'features': feature_list,  # unionã‚’è¨­å®š
                        }
                        
                        # Get the list of cell types
                        cell_types = cell_list
                        
                        # Run permutation test
                        results = run_permutation_cellchat_64core(
                            adata1=adata1,
                            adata2=adata2,
                            groupby=celltype_key,
                            db=cellchatdb,
                            cell_types=cell_types,
                            n_perm=n_perms,           # ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ•°
                            n_jobs=n_cpus,            # 64ã‚³ã‚¢ã®75%
                            use_gpu=True,         # GPUä½¿ç”¨
                            gpu_precision=True,   # float64ç²¾åº¦å›ºå®š
                            hybrid_mode=True,     # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹åŒ–
                            gpu_ratio=0.3,        # GPUæ¯”ç‡
                            optimize_memory=True,
                            **cellchat_params
                        )
                        
                        # Store results in session state
                        st.session_state.perm_results = perm_results
                        st.session_state.cell_types = cell_types
                        st.session_state.group1 = group1
                        st.session_state.group2 = group2
                        
                        # Mark analysis as completed
                        st.session_state.analysis_completed = True
                        st.session_state.current_mode = "visualization"
                        
                        st.success(f"Permutation analysis completed successfully with {n_perms} permutations!")
                        
                        # Force a rerun to switch to visualization mode
                        st.rerun()
                
                except Exception as e:
                    st.error(f"Error during permutation analysis: {str(e)}")
                    st.exception(e)
                    st.session_state.current_mode = "setup"
            
            elif st.session_state.current_mode == "visualization" or st.session_state.analysis_completed:
                # Add visualization controls
                st.header("Visualization Settings")
                
                # Initialize significance threshold in session state if not exists
                if 'significance_threshold' not in st.session_state:
                    st.session_state.significance_threshold = 0.05
                
                # Function to update threshold in session state
                def update_threshold():
                    st.session_state.significance_threshold = st.session_state.threshold_slider
                
                significance_threshold = st.slider(
                    "Default significance threshold:",
                    min_value=0.001,
                    max_value=0.1,
                    value=st.session_state.significance_threshold,
                    step=0.001,
                    key="threshold_slider",
                    on_change=update_threshold
                )
                
                # Add a separate button for visualizing results
                if st.button("Show Visualizations", key="show_viz") or 'show_visualization' in st.session_state:
                    if 'perm_results' in st.session_state:
                        # Display results
                        perm_results = st.session_state.perm_results
                        cell_types = st.session_state.cell_types
                        group1 = st.session_state.group1
                        group2 = st.session_state.group2
                        
                        # Set flag to indicate visualization is shown
                        st.session_state.show_visualization = True
                        
                        # Display visualizations
                        display_visualizations(
                            perm_results=perm_results,
                            cell_types=cell_types,
                            group1=group1,
                            group2=group2,
                            significance_threshold=st.session_state.significance_threshold
                        )
                        
                        # Offer to save the entire permutation results
                        st.markdown("---")
                        st.header("Save Permutation Results")
                        
                        if st.button("Prepare permutation results for download"):
                            try:
                                # Prepare data for download
                                result_data = {
                                    'group1': group1,
                                    'group2': group2,
                                    'n_perms': st.session_state.analysis_parameters['n_perms'],
                                    'actual_diff': perm_results['actual_diff'],
                                    'adjusted_p_values': perm_results['adjusted_p_values'],
                                    'parameters': cellchat_params,
                                    'timestamp': time.time(),
                                    'file_name': uploaded_file.name
                                }
                                
                                # Serialize to file
                                with st.spinner("Serializing permutation results..."):
                                    pickled_data = pickle.dumps(result_data)
                                    
                                    # Offer download
                                    st.download_button(
                                        label="Download permutation analysis results",
                                        data=pickled_data,
                                        file_name=f"cellchat_permutation_{group1}_vs_{group2}_{n_perms}perms.pkl",
                                        mime="application/octet-stream"
                                    )
                                    
                                    st.success("Permutation results prepared for download!")
                            except Exception as e:
                                st.error(f"Error preparing results for download: {str(e)}")
                                st.exception(e)
                    else:
                        st.error("No results to visualize. Please run the analysis first.")
                
                # Add a button to reset and run a new analysis
                if st.button('Reset Analysis'):
                    # Reset all relevant session state variables
                    st.session_state.analysis_completed = False
                    st.session_state.current_mode = "setup"
                    if 'perm_results' in st.session_state:
                        del st.session_state.perm_results
                    if 'show_visualization' in st.session_state:
                        del st.session_state.show_visualization
                    if 'selected_pathway' in st.session_state:
                        del st.session_state.selected_pathway
                    if 'significance_threshold' in st.session_state:
                        del st.session_state.significance_threshold
                    st.rerun()
        
        except Exception as e:
            st.error(f"Error loading file: {str(e)}")
            st.exception(e)