import streamlit as st
from paperqa import Docs, Settings, agent_query
import os
import tempfile
from dotenv import load_dotenv
from datetime import datetime
import json
import requests
import urllib.parse
import time
from requests.auth import HTTPBasicAuth, HTTPDigestAuth
from urllib.parse import urljoin, urlparse
import re
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import asyncio
import nest_asyncio

# Streamlitã§éåŒæœŸã‚’ä½¿ã†ãŸã‚ã®è¨­å®š
nest_asyncio.apply()

# Failed DOI tracking
if 'failed_dois' not in st.session_state:
    st.session_state.failed_dois = {'elsevier': [], 'other': []}

# Streamlitã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥é–¢æ•°
@st.cache_data
def load_api_keys():
    """APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    env_path = "./.env"
    
    if not os.path.exists(env_path):
        st.error(f"âŒ .envãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {os.path.abspath(env_path)}")
        return {"OpenAI": False, "Anthropic/Claude": False, "Google Gemini": False}
    
    success = load_dotenv(env_path)
    if not success:
        st.error(f"âŒ .envãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {os.path.abspath(env_path)}")
    else:
        st.success(f"âœ… .envãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {os.path.abspath(env_path)}")
    
    return {
        "OpenAI": os.getenv("OPENAI_API_KEY") is not None,
        "Anthropic/Claude": True,  # API key is hardcoded
        "Google Gemini": os.getenv("GEMINI_API_KEY") is not None
    }

def get_paper_directory():
    """è«–æ–‡ä¿å­˜ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’å–å¾—"""
    paper_dir = os.path.join(os.getcwd(), "paperqa2", "papers")
    os.makedirs(paper_dir, exist_ok=True)
    return paper_dir

def track_failed_doi(doi, reason="Download failed"):
    """å¤±æ•—ã—ãŸDOIã‚’è¨˜éŒ²ã™ã‚‹"""
    # Check if it's Elsevier/ScienceDirect
    is_elsevier = False
    try:
        if 'doi.org/10.1016' in doi or 'sciencedirect.com' in doi or 'elsevier.com' in doi:
            is_elsevier = True
    except:
        pass
    
    failed_entry = {
        'doi': doi,
        'reason': reason,
        'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    
    if is_elsevier:
        if failed_entry not in st.session_state.failed_dois['elsevier']:
            st.session_state.failed_dois['elsevier'].append(failed_entry)
    else:
        if failed_entry not in st.session_state.failed_dois['other']:
            st.session_state.failed_dois['other'].append(failed_entry)

def get_comprehensive_pdf_url(paper):
    """åŒ…æ‹¬çš„PDF URLå–å¾—ï¼ˆè¤‡æ•°ã®æ–¹æ³•ã‚’è©¦è¡Œï¼‰"""
    urls_to_try = []
    
    # 1. ç›´æ¥PDF URLï¼ˆæœ€å„ªå…ˆï¼‰
    if paper.get('pdf_url'):
        urls_to_try.append(('direct_pdf', paper['pdf_url']))
    
    # 2. PMC PDF URL - è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
    if paper.get('pmcid'):
        pmcid = paper['pmcid']
        pmc_urls = [
            f"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/pdf/",
            f"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/pdf/{pmcid}.pdf"
        ]
        for pmc_url in pmc_urls:
            urls_to_try.append(('pmc_pdf', pmc_url))
    
    # 3. DOI ãƒ™ãƒ¼ã‚¹å–å¾—
    if paper.get('doi'):
        doi_url = f"https://doi.org/{paper['doi']}"
        urls_to_try.append(('doi_redirect', doi_url))
    
    return urls_to_try

def download_paper_comprehensive(paper):
    """è¤‡æ•°ã®URLã‚’è©¦è¡Œã—ã¦PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    urls_to_try = get_comprehensive_pdf_url(paper)
    
    for method, url in urls_to_try:
        try:
            st.write(f"ğŸ”„ Trying {method}: {url[:50]}...")
            
            # åŸºæœ¬çš„ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            if method == 'direct_pdf' or method == 'pmc_pdf':
                # ç›´æ¥PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                response = requests.get(url, headers=headers, timeout=30, allow_redirects=True)
                if response.content.startswith(b'%PDF'):
                    st.success(f"âœ… {method} success!")
                    return response.content
                else:
                    st.warning(f"âš ï¸ {method} failed - not PDF content")
            
            elif method == 'doi_redirect':
                # DOIãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã‚’è¿½è·¡ã—ã¦Publisher siteã¸
                pdf_content = download_from_publisher_site(url)
                if pdf_content:
                    st.success(f"âœ… {method} success!")
                    return pdf_content
                else:
                    st.warning(f"âš ï¸ {method} failed")
                    
        except Exception as e:
            st.warning(f"âš ï¸ {method} failed: {str(e)[:100]}...")
            continue
    
    # All methods failed - track the DOI
    doi = paper.get('doi', url if 'doi.org' in url else 'Unknown DOI')
    track_failed_doi(doi, "All download methods failed")
    return None

def download_from_publisher_site(publisher_url):
    """Publisher siteã‹ã‚‰PDFã‚’æŠ½å‡ºï¼ˆç°¡ç´ åŒ–ç‰ˆï¼‰"""
    try:
        session = requests.Session()
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9'
        }
        
        # DOIãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã®å ´åˆ
        if 'doi.org' in publisher_url:
            response = session.get(publisher_url, headers=headers, allow_redirects=True, timeout=15)
            final_url = response.url
            st.write(f"ğŸ”„ Redirected to: {final_url[:50]}...")
            
            # Skip Elsevier/ScienceDirect special handling - treat as generic
            if 'sciencedirect.com' in final_url or 'elsevier.com' in final_url:
                st.write("ğŸ”„ Elsevier site detected - using generic approach...")
                track_failed_doi(publisher_url, "Elsevier site - special handling removed")
                return None
        else:
            response = session.get(publisher_url, timeout=15, headers=headers)
        
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Generic PDF link detection
        pdf_links = []
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            text = link.get_text().strip().lower()
            
            if ('pdf' in href.lower() or 
                ('pdf' in text and ('download' in text or 'full' in text)) or
                ('download' in text and 'article' in text)):
                if href.startswith('/'):
                    href = urljoin(publisher_url, href)
                elif not href.startswith('http'):
                    domain = urlparse(publisher_url).netloc
                    href = urljoin(f"https://{domain}", href)
                pdf_links.append(href)
        
        # Try downloading PDFs
        for pdf_url in pdf_links[:3]:  # Try first 3 links
            try:
                st.write(f"ğŸ”„ Trying PDF: {pdf_url[:50]}...")
                pdf_response = session.get(pdf_url, timeout=30, headers=headers, allow_redirects=True)
                
                if (pdf_response.content and 
                    len(pdf_response.content) > 1000 and  
                    pdf_response.content.startswith(b'%PDF')):
                    return pdf_response.content
                    
            except Exception as e:
                st.write(f"âš ï¸ PDF download failed: {str(e)[:50]}...")
        
        # No valid PDF found
        track_failed_doi(publisher_url, "No valid PDF found on publisher site")
        return None
        
    except Exception as e:
        st.error(f"âŒ Publisher site error: {str(e)[:100]}...")
        track_failed_doi(publisher_url, f"Publisher site error: {str(e)[:50]}")
        return None

def download_pmc_pdf(pmcid):
    """PMCã‹ã‚‰PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³è©¦è¡Œï¼‰"""
    pmc_patterns = [
        f"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/pdf/",
        f"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/pdf/{pmcid}.pdf",
        f"https://europepmc.org/articles/{pmcid}?pdf=render"
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    for pattern in pmc_patterns:
        try:
            st.write(f"ğŸ”„ Trying PMC PDF: {pattern}")
            response = requests.get(pattern, headers=headers, timeout=30)
            
            if response.content.startswith(b'%PDF'):
                st.success("âœ… PMC PDF download successful!")
                return response.content
            else:
                st.write(f"âš ï¸ Not PDF content: {len(response.content)} bytes")
                
        except Exception as e:
            st.write(f"âš ï¸ PMC pattern failed: {str(e)[:50]}...")
    
    track_failed_doi(f"PMC:{pmcid}", "PMC PDF download failed")
    return None

def search_pubmed_papers(query_terms):
    """PubMed APIã‚’ä½¿ã£ã¦è«–æ–‡ã‚’æ¤œç´¢"""
    try:
        base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
        
        # ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
        search_query = " AND ".join([f'("{term}")' for term in query_terms if term.strip()])
        
        # æ¤œç´¢å®Ÿè¡Œ
        search_params = {
            'db': 'pubmed',
            'term': search_query,
            'retmax': 50,
            'retmode': 'json'
        }
        
        search_response = requests.get(f"{base_url}esearch.fcgi", params=search_params, timeout=30)
        search_data = search_response.json()
        
        if 'esearchresult' not in search_data or not search_data['esearchresult']['idlist']:
            return []
        
        pmids = search_data['esearchresult']['idlist']
        
        # è©³ç´°æƒ…å ±ã‚’å–å¾—
        fetch_params = {
            'db': 'pubmed',
            'id': ','.join(pmids),
            'retmode': 'xml'
        }
        
        fetch_response = requests.get(f"{base_url}efetch.fcgi", params=fetch_params, timeout=30)
        
        # XMLãƒ‘ãƒ¼ã‚¹
        root = ET.fromstring(fetch_response.content)
        
        papers = []
        for article in root.findall('.//PubmedArticle'):
            paper_info = extract_paper_info(article)
            if paper_info:
                papers.append(paper_info)
        
        return papers
        
    except Exception as e:
        st.error(f"PubMedæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def extract_paper_info(article):
    """XMLã‹ã‚‰è«–æ–‡æƒ…å ±ã‚’æŠ½å‡º"""
    try:
        # ã‚¿ã‚¤ãƒˆãƒ«
        title_elem = article.find('.//ArticleTitle')
        title = title_elem.text if title_elem is not None else "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜"
        
        # DOI
        doi = None
        for article_id in article.findall('.//ArticleId'):
            if article_id.get('IdType') == 'doi':
                doi = article_id.text
                break
        
        # PMCID
        pmcid = None
        for article_id in article.findall('.//ArticleId'):
            if article_id.get('IdType') == 'pmc':
                pmcid = article_id.text
                break
        
        # è‘—è€…
        authors = []
        for author in article.findall('.//Author'):
            last_name = author.find('LastName')
            first_name = author.find('ForeName')
            if last_name is not None and first_name is not None:
                authors.append(f"{first_name.text} {last_name.text}")
        
        # é›‘èªŒå
        journal_elem = article.find('.//Title')
        journal = journal_elem.text if journal_elem is not None else "é›‘èªŒåä¸æ˜"
        
        # å‡ºç‰ˆå¹´
        pub_year = None
        year_elem = article.find('.//PubDate/Year')
        if year_elem is not None:
            pub_year = year_elem.text
        
        return {
            'title': title,
            'authors': ', '.join(authors[:3]),  # æœ€åˆã®3äººã®è‘—è€…
            'journal': journal,
            'year': pub_year,
            'doi': doi,
            'pmcid': pmcid
        }
        
    except Exception as e:
        st.error(f"è«–æ–‡æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return None

def create_paper_entry_display(paper, index):
    """å€‹ã€…ã®è«–æ–‡ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã®è¡¨ç¤º"""
    with st.container():
        col1, col2 = st.columns([4, 1])
        
        with col1:
            st.write(f"**{paper['title']}**")
            if paper.get('authors'):
                st.write(f"*è‘—è€…:* {paper['authors']}")
            if paper.get('journal') and paper.get('year'):
                st.write(f"*é›‘èªŒ:* {paper['journal']} ({paper['year']})")
            
            # DOI/PMCIDã®è¡¨ç¤º
            identifiers = []
            if paper.get('doi'):
                identifiers.append(f"DOI: {paper['doi']}")
            if paper.get('pmcid'):
                identifiers.append(f"PMC: {paper['pmcid']}")
            if identifiers:
                st.write(f"*è­˜åˆ¥å­:* {' | '.join(identifiers)}")
        
        with col2:
            download_key = f"download_{index}"
            if st.button("ğŸ“„ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", key=download_key):
                with st.spinner(f"è«–æ–‡ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                    pdf_content = download_paper_comprehensive(paper)
                    
                    if pdf_content:
                        # PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
                        paper_dir = get_paper_directory()
                        safe_title = re.sub(r'[^\w\s-]', '', paper['title'][:50])
                        filename = f"{safe_title}_{int(time.time())}.pdf"
                        filepath = os.path.join(paper_dir, filename)
                        
                        with open(filepath, 'wb') as f:
                            f.write(pdf_content)
                        
                        st.success(f"âœ… è«–æ–‡ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ: {filename}")
                        
                        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‚’æä¾›
                        st.download_button(
                            label="ğŸ’¾ PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=pdf_content,
                            file_name=filename,
                            mime="application/pdf"
                        )
                    else:
                        st.error("âŒ PDFã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")

def display_failed_dois():
    """å¤±æ•—ã—ãŸDOIã®è¡¨ç¤º"""
    st.markdown("---")
    st.header("ğŸ“Š ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸè«–æ–‡")
    
    elsevier_count = len(st.session_state.failed_dois['elsevier'])
    other_count = len(st.session_state.failed_dois['other'])
    
    if elsevier_count == 0 and other_count == 0:
        st.success("âœ… å…¨ã¦ã®è«–æ–‡ãŒæ­£å¸¸ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸï¼")
        return
    
    st.write(f"**å¤±æ•—ã—ãŸè«–æ–‡æ•°:** Elsevier: {elsevier_count}ä»¶, ãã®ä»–: {other_count}ä»¶")
    
    # Elsevier failures
    if elsevier_count > 0:
        st.subheader("ğŸ”¬ Elsevier/ScienceDirect")
        st.info("Elsevierã¯ç‰¹æ®Šãªå‡¦ç†ãŒå¿…è¦ãªãŸã‚ã€æ‰‹å‹•ã§ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ãã ã•ã„ã€‚")
        
        for i, failed in enumerate(st.session_state.failed_dois['elsevier']):
            with st.expander(f"Elsevierè«–æ–‡ {i+1}: {failed['doi'][:50]}..."):
                st.write(f"**DOI/URL:** {failed['doi']}")
                st.write(f"**å¤±æ•—ç†ç”±:** {failed['reason']}")
                st.write(f"**æ™‚åˆ»:** {failed['timestamp']}")
                
                # Create clickable link
                if failed['doi'].startswith('http'):
                    link_url = failed['doi']
                elif failed['doi'].startswith('10.'):
                    link_url = f"https://doi.org/{failed['doi']}"
                else:
                    link_url = failed['doi']
                
                st.markdown(f"ğŸ”— [è«–æ–‡ã«ã‚¢ã‚¯ã‚»ã‚¹]({link_url})")
    
    # Other failures
    if other_count > 0:
        st.subheader("ğŸ“š ãã®ä»–ã®å‡ºç‰ˆç¤¾")
        
        for i, failed in enumerate(st.session_state.failed_dois['other']):
            with st.expander(f"è«–æ–‡ {i+1}: {failed['doi'][:50]}..."):
                st.write(f"**DOI/URL:** {failed['doi']}")
                st.write(f"**å¤±æ•—ç†ç”±:** {failed['reason']}")
                st.write(f"**æ™‚åˆ»:** {failed['timestamp']}")
                
                # Create clickable link
                if failed['doi'].startswith('http'):
                    link_url = failed['doi']
                elif failed['doi'].startswith('10.'):
                    link_url = f"https://doi.org/{failed['doi']}"
                else:
                    link_url = failed['doi']
                
                st.markdown(f"ğŸ”— [è«–æ–‡ã«ã‚¢ã‚¯ã‚»ã‚¹]({link_url})")
    
    # Clear failed DOIs button
    if st.button("ğŸ§¹ å¤±æ•—ãƒªã‚¹ãƒˆã‚’ã‚¯ãƒªã‚¢"):
        st.session_state.failed_dois = {'elsevier': [], 'other': []}
        st.rerun()

def main():
    st.title("ğŸ“š PaperQA2 - å­¦è¡“è«–æ–‡æ¤œç´¢ãƒ»åˆ†æã‚·ã‚¹ãƒ†ãƒ ")
    
    # API ã‚­ãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
    api_status = load_api_keys()
    
    with st.sidebar:
        st.subheader("ğŸ”‘ API Status")
        for service, available in api_status.items():
            if available:
                st.success(f"âœ… {service}")
            else:
                st.error(f"âŒ {service}")
        
        st.markdown("---")
        st.subheader("ğŸ“ Papers Directory")
        paper_dir = get_paper_directory()
        st.info(f"ğŸ“‚ {paper_dir}")
        
        # Count papers
        try:
            paper_files = [f for f in os.listdir(paper_dir) if f.endswith('.pdf')]
            st.write(f"ğŸ’¾ ä¿å­˜æ¸ˆã¿è«–æ–‡: {len(paper_files)}ä»¶")
        except:
            st.write("ğŸ’¾ ä¿å­˜æ¸ˆã¿è«–æ–‡: 0ä»¶")
    
    # ãƒ¡ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼faces
    tab1, tab2, tab3 = st.tabs(["ğŸ” è«–æ–‡æ¤œç´¢", "ğŸ’¬ è«–æ–‡åˆ†æ", "ğŸ“Š çµ±è¨ˆ"])
    
    with tab1:
        st.header("ğŸ” è«–æ–‡æ¤œç´¢")
        
        # æ¤œç´¢ã‚¯ã‚¨ãƒªå…¥åŠ›
        st.subheader("æ¤œç´¢æ¡ä»¶")
        query_input = st.text_area(
            "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆ1è¡Œã«ã¤ã1ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰:",
            placeholder="ä¾‹:\ncancer treatment\nmachine learning\nCOVID-19",
            height=100
        )
        
        if st.button("ğŸ” è«–æ–‡ã‚’æ¤œç´¢", type="primary"):
            if not query_input.strip():
                st.warning("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                return
            
            query_terms = [term.strip() for term in query_input.strip().split('\n') if term.strip()]
            
            with st.spinner("PubMedã§è«–æ–‡ã‚’æ¤œç´¢ä¸­..."):
                papers = search_pubmed_papers(query_terms)
            
            if papers:
                st.success(f"âœ… {len(papers)}ä»¶ã®è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
                
                for i, paper in enumerate(papers):
                    create_paper_entry_display(paper, i)
                    if i < len(papers) - 1:
                        st.markdown("---")
            else:
                st.warning("è©²å½“ã™ã‚‹è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    
    with tab2:
        st.header("ğŸ’¬ è«–æ–‡åˆ†æ")
        
        paper_dir = get_paper_directory()
        try:
            paper_files = [f for f in os.listdir(paper_dir) if f.endswith('.pdf')]
            
            if not paper_files:
                st.info("ã¾ãšã€Œè«–æ–‡æ¤œç´¢ã€ã‚¿ãƒ–ã§è«–æ–‡ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            else:
                st.write(f"ğŸ“š åˆ©ç”¨å¯èƒ½ãªè«–æ–‡: {len(paper_files)}ä»¶")
                
                # è³ªå•å…¥åŠ›
                question = st.text_input(
                    "è«–æ–‡ã«é–¢ã™ã‚‹è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:",
                    placeholder="ä¾‹: ã“ã®ç ”ç©¶ã®ä¸»ãªçµæœã¯ä½•ã§ã™ã‹ï¼Ÿ"
                )
                
                if st.button("ğŸ’¬ è³ªå•ã™ã‚‹", type="primary"):
                    if not question:
                        st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                    else:
                        with st.spinner("è«–æ–‡ã‚’åˆ†æä¸­..."):
                            try:
                                settings = Settings()
                                docs = Docs(docs_path=paper_dir)
                                
                                # éåŒæœŸé–¢æ•°ã‚’åŒæœŸçš„ã«å®Ÿè¡Œ
                                loop = asyncio.new_event_loop()
                                asyncio.set_event_loop(loop)
                                result = loop.run_until_complete(
                                    agent_query(question, docs=docs, settings=settings)
                                )
                                
                                st.success("âœ… åˆ†æå®Œäº†")
                                st.markdown("### ğŸ“‹ å›ç­”")
                                st.write(result.answer)
                                
                                if result.references:
                                    st.markdown("### ğŸ“š å‚è€ƒæ–‡çŒ®")
                                    for ref in result.references:
                                        st.write(f"- {ref}")
                                
                            except Exception as e:
                                st.error(f"åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        except Exception as e:
            st.error(f"è«–æ–‡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚¢ã‚¯ã‚»ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    
    with tab3:
        st.header("ğŸ“Š çµ±è¨ˆæƒ…å ±")
        
        paper_dir = get_paper_directory()
        try:
            paper_files = [f for f in os.listdir(paper_dir) if f.endswith('.pdf')]
            st.metric("ä¿å­˜æ¸ˆã¿è«–æ–‡æ•°", len(paper_files))
            
            if paper_files:
                st.subheader("ğŸ“ ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«")
                for file in paper_files[:10]:  # Show first 10 files
                    st.write(f"ğŸ“„ {file}")
                
                if len(paper_files) > 10:
                    st.write(f"... ãŠã‚ˆã³ {len(paper_files) - 10} ä»¶ã®è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«")
        
        except Exception as e:
            st.error(f"çµ±è¨ˆæƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    
    # Display failed DOIs at the bottom
    display_failed_dois()

if __name__ == "__main__":
    main()